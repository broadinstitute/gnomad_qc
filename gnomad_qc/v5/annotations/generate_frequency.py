"""
Script to generate frequency data and age histograms for gnomAD v5.

This script calculates variant frequencies and generates age histograms for both gnomAD v4
samples that will be removed in v5 and All of Us samples that will be removed. It accounts
for samples that change ancestry between versions and handles the differential analysis
approach to avoid densifying the full dataset.

The script uses the AN (allele number) data generated by the coverage computation script
and accounts for samples that will be dropped due to relatedness filtering or ancestry changes.
"""

import argparse
import logging
from typing import Dict, List, Optional, Tuple, Union

import hail as hl
from gnomad.resources.grch38.gnomad import (
    DOWNSAMPLINGS,
    GEN_ANC_GROUPS_TO_REMOVE_FOR_GRPMAX,
)
from gnomad.sample_qc.sex import adjusted_sex_ploidy_expr
from gnomad.utils.annotations import (
    age_hists_expr,
    annotate_downsamplings,
    bi_allelic_site_inbreeding_expr,
    build_freq_stratification_list,
    compute_freq_by_strata,
    faf_expr,
    gen_anc_faf_max_expr,
    generate_freq_group_membership_array,
    get_adj_expr,
    grpmax_expr,
    merge_freq_arrays,
    merge_histograms,
    qual_hist_expr,
    set_xx_y_metrics_to_na_expr,
)
from gnomad.utils.filtering import filter_arrays_by_meta
from gnomad.utils.release import make_freq_index_dict_from_meta
from gnomad.utils.slack import slack_notifications
from gnomad.utils.vcf import SORT_ORDER

from gnomad_qc.resource_utils import (
    PipelineResourceCollection,
    PipelineStepResourceCollection,
)
from gnomad_qc.slack_creds import slack_token
from gnomad_qc.v5.resources.basics import get_logging_path
from gnomad_qc.v5.resources.constants import WORKSPACE_BUCKET
from gnomad_qc.v5.resources.meta import project_meta
from gnomad_qc.v5.resources.sample_qc import (
    get_joint_qc,
    hard_filtered_samples,
    related_samples_to_drop,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s: %(message)s",
    datefmt="%m/%d/%Y %I:%M:%S %p",
)
logger = logging.getLogger("gnomAD_v5_frequency")
logger.setLevel(logging.INFO)

AGE_HISTS = [
    "age_hist_het",
    "age_hist_hom",
]
"""Age histograms to compute and keep on the frequency Table."""

QUAL_HISTS = [
    "gq_hist_all",
    "dp_hist_all",
    "gq_hist_alt",
    "dp_hist_alt",
    "ab_hist_alt",
]
"""Quality histograms to compute and keep on the frequency Table."""

FREQ_ROW_FIELDS = [
    "freq",
    "qual_hists",
    "raw_qual_hists",
    "age_hists",
]
"""
List of top level row and global annotations that we want on the frequency HT.
"""

FREQ_GLOBAL_FIELDS = [
    "downsamplings",
    "freq_meta",
    "age_distribution",
    "freq_index_dict",
    "freq_meta_sample_count",
]
"""
List of final global annotations created from dense data that we want on the frequency HT.
"""


def get_freq_resources(
    overwrite: bool = False,
    test: Optional[bool] = False,
    data_set: str = "gnomad",
) -> PipelineResourceCollection:
    """
    Get frequency resources for v5.

    :param overwrite: Whether to overwrite existing files.
    :param test: Whether to use test resources.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency resources.
    """
    freq_pipeline = PipelineResourceCollection(
        pipeline_name="v5_frequency",
        overwrite=overwrite,
    )

    # Define output resources for frequency calculations
    freq_output_resources = {
        "freq_ht": get_freq_ht(test=test, data_set=data_set),
        "age_hist_ht": get_age_hist_ht(test=test, data_set=data_set),
    }

    run_freq_calculations = PipelineStepResourceCollection(
        "--run-frequency-calculations",
        output_resources=freq_output_resources,
    )

    freq_pipeline.add_steps(
        {
            "run_freq_calculations": run_freq_calculations,
        }
    )

    return freq_pipeline


def get_freq_ht(test: bool = False, data_set: str = "gnomad") -> TableResource:
    """
    Get the frequency Table resource for v5.

    :param test: Whether to use test resources.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency Table resource.
    """
    if test:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/testing/frequency/{data_set}_freq_test.ht"
        )
    else:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/frequency/{data_set}_freq.ht"
        )


def get_age_hist_ht(test: bool = False, data_set: str = "gnomad") -> TableResource:
    """
    Get the age histogram Table resource for v5.

    :param test: Whether to use test resources.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Age histogram Table resource.
    """
    if test:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/testing/frequency/{data_set}_age_hist_test.ht"
        )
    else:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/frequency/{data_set}_age_hist.ht"
        )


def identify_samples_to_remove(
    meta_ht: hl.Table,
    relatedness_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Identify samples that will be removed from v5 due to relatedness or ancestry changes.

    :param meta_ht: Metadata Table with sample information.
    :param relatedness_ht: Relatedness Table with samples to drop.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Table with samples to remove and their reasons.
    """
    logger.info(f"Identifying samples to remove for {data_set} dataset...")

    # Get samples to drop due to relatedness
    samples_to_drop = related_samples_to_drop(test=False).ht()

    # Join with metadata to get ancestry information
    meta_with_drops = meta_ht.annotate(
        will_be_dropped=samples_to_drop[meta_ht.key].s.is_defined()
    )

    # For gnomAD, we need to compare v4 vs v5 ancestry assignments
    if data_set == "gnomad":
        # TODO: Add logic to compare v4 vs v5 ancestry assignments
        # This would require loading v4 metadata and comparing ancestry assignments
        pass

    # For All of Us, we need to identify samples that will be filtered out
    elif data_set == "aou":
        # TODO: Add logic to identify AoU samples that will be filtered
        # This would depend on the specific filtering criteria for AoU
        pass

    return meta_with_drops


def calculate_frequency_for_removed_samples(
    mt: hl.MatrixTable,
    samples_to_remove: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Calculate frequency data for samples that will be removed.

    :param mt: Input MatrixTable.
    :param samples_to_remove: Table with samples to remove.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency Table for removed samples.
    """
    logger.info(f"Calculating frequency for removed samples in {data_set} dataset...")

    # Filter to only samples that will be removed
    removed_samples = samples_to_remove.filter(samples_to_remove.will_be_dropped)
    mt_removed = mt.filter_cols(removed_samples[mt.col_key].s.is_defined())

    # Build frequency stratification
    strata_expr = build_freq_stratification_list(
        sex_expr=mt_removed.meta.sex_karyotype,
        gen_anc_expr=mt_removed.meta.pop,
        additional_strata_expr=[
            {"data_set": data_set},
            {"data_set": data_set, "pop": mt_removed.meta.pop},
        ],
    )

    # Generate frequency group membership
    group_membership_ht = generate_freq_group_membership_array(
        mt_removed.cols(),
        strata_expr,
        downsamplings=hl.literal(DOWNSAMPLINGS),
    )

    # Compute frequency by strata
    freq_ht = compute_freq_by_strata(
        mt_removed,
        group_membership_ht[mt_removed.col_key].group_membership,
        strata_expr,
    )

    return freq_ht


def calculate_age_histograms(
    mt: hl.MatrixTable,
    samples_to_remove: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Calculate age histograms for samples that will be removed.

    :param mt: Input MatrixTable.
    :param samples_to_remove: Table with samples to remove.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Age histogram Table for removed samples.
    """
    logger.info(
        f"Calculating age histograms for removed samples in {data_set} dataset..."
    )

    # Filter to only samples that will be removed
    removed_samples = samples_to_remove.filter(samples_to_remove.will_be_dropped)
    mt_removed = mt.filter_cols(removed_samples[mt.col_key].s.is_defined())

    # Calculate age histograms
    age_hist_expr = age_hists_expr(
        mt_removed.GT,
        mt_removed.meta.age,
        mt_removed.meta.sex_karyotype,
    )

    ht = mt_removed.aggregate_rows(
        age_hist_het=hl.agg.explode(age_hist_expr.age_hist_het),
        age_hist_hom=hl.agg.explode(age_hist_expr.age_hist_hom),
    )

    return ht


def merge_frequency_data(
    gnomad_freq_ht: hl.Table,
    aou_freq_ht: hl.Table,
    test: bool = False,
) -> hl.Table:
    """
    Merge frequency data from gnomAD and All of Us datasets.

    :param gnomad_freq_ht: Frequency Table for gnomAD removed samples.
    :param aou_freq_ht: Frequency Table for AoU removed samples.
    :param test: Whether to use test resources.
    :return: Merged frequency Table.
    """
    logger.info("Merging frequency data from gnomAD and All of Us datasets...")

    # Combine frequency arrays
    freq_arrays = [gnomad_freq_ht.freq, aou_freq_ht.freq]
    freq_meta = [gnomad_freq_ht.freq_meta, aou_freq_ht.freq_meta]

    merged_freq, merged_freq_meta = merge_freq_arrays(
        freq_arrays,
        freq_meta,
    )

    # Merge histograms
    hist_structs = {
        "qual_hists": QUAL_HISTS,
        "raw_qual_hists": QUAL_HISTS,
        "age_hists": AGE_HISTS,
    }

    hists_expr = {
        hist_struct: hl.struct(
            **{
                h: merge_histograms(
                    [gnomad_freq_ht[hist_struct][h], aou_freq_ht[hist_struct][h]]
                )
                for h in hists
            }
        )
        for hist_struct, hists in hist_structs.items()
    }

    # Create merged Table
    merged_ht = gnomad_freq_ht.select(
        freq=merged_freq,
        **hists_expr,
    )

    merged_ht = merged_ht.annotate_globals(
        freq_meta=merged_freq_meta,
        freq_index_dict=make_freq_index_dict_from_meta(merged_freq_meta),
    )

    return merged_ht


def main(args):
    """Generate frequency data and age histograms for v5."""
    hl.init(
        log="/v5_frequency.log",
        default_reference="GRCh38",
        tmp_dir=f"gs://{WORKSPACE_BUCKET}/tmp/4_day",
    )

    # SSA Logs are easier to troubleshoot with.
    hl._set_flags(use_ssa_logs="1")

    test = args.test
    overwrite = args.overwrite
    data_set = args.data_set

    try:
        if args.run_frequency_calculations:
            logger.info(f"Running frequency calculations for {data_set} dataset...")

            # Load metadata and relatedness data
            meta_ht = project_meta.ht()
            relatedness_ht = related_samples_to_drop(test=test).ht()

            # Identify samples to remove
            samples_to_remove = identify_samples_to_remove(
                meta_ht, relatedness_ht, data_set
            )

            # Load joint QC MatrixTable
            joint_qc_mt = get_joint_qc().mt()

            if test:
                logger.info("Filtering MT to the first 2 partitions for testing...")
                joint_qc_mt = joint_qc_mt._filter_partitions(range(2))

            # Calculate frequency for removed samples
            freq_ht = calculate_frequency_for_removed_samples(
                joint_qc_mt, samples_to_remove, data_set
            )

            # Calculate age histograms
            age_hist_ht = calculate_age_histograms(
                joint_qc_mt, samples_to_remove, data_set
            )

            # Write results
            freq_ht.write(
                get_freq_ht(test=test, data_set=data_set).path, overwrite=overwrite
            )
            age_hist_ht.write(
                get_age_hist_ht(test=test, data_set=data_set).path, overwrite=overwrite
            )

        if args.merge_datasets:
            logger.info("Merging frequency data from both datasets...")

            # Load frequency data from both datasets
            gnomad_freq_ht = get_freq_ht(test=test, data_set="gnomad").ht()
            aou_freq_ht = get_freq_ht(test=test, data_set="aou").ht()

            # Merge frequency data
            merged_freq_ht = merge_frequency_data(
                gnomad_freq_ht, aou_freq_ht, test=test
            )

            # Write merged results
            merged_freq_ht.write(
                get_freq_ht(test=test, data_set="joint").path, overwrite=overwrite
            )

    finally:
        hl.copy_log(get_logging_path("v5_frequency"))


def get_script_argument_parser() -> argparse.ArgumentParser:
    """Get script argument parser."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--overwrite", help="Overwrite existing hail Tables.", action="store_true"
    )
    parser.add_argument(
        "--test",
        help="Filter to the first 2 partitions for testing.",
        action="store_true",
    )
    parser.add_argument(
        "--data-set",
        help="Dataset to process. One of 'gnomad', 'aou', or 'joint'.",
        type=str,
        choices=["gnomad", "aou", "joint"],
        default="gnomad",
    )
    parser.add_argument(
        "--run-frequency-calculations",
        help="Run frequency calculations for the specified dataset.",
        action="store_true",
    )
    parser.add_argument(
        "--merge-datasets",
        help="Merge frequency data from gnomAD and All of Us datasets.",
        action="store_true",
    )

    return parser


if __name__ == "__main__":
    parser = get_script_argument_parser()
    args = parser.parse_args()
    main(args)
