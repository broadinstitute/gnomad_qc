"""
Script to generate frequency data and age histograms for gnomAD v5.

This script calculates variant frequencies and generates age histograms for both gnomAD v4
samples that will be removed in v5 and All of Us samples that will be removed. It accounts
for samples that change ancestry between versions and handles the differential analysis
approach to avoid densifying the full dataset.

The script uses the AN (allele number) data generated by the coverage computation script
and accounts for samples that will be dropped due to relatedness filtering or ancestry changes.
"""

import argparse
import logging
from typing import Dict, List, Optional, Tuple, Union

import hail as hl
from gnomad.resources.grch38.gnomad import (
    DOWNSAMPLINGS,
    GEN_ANC_GROUPS_TO_REMOVE_FOR_GRPMAX,
)
from gnomad.resources.resource_utils import TableResource
from gnomad.sample_qc.sex import adjusted_sex_ploidy_expr
from gnomad.utils.annotations import (
    age_hists_expr,
    annotate_downsamplings,
    bi_allelic_site_inbreeding_expr,
    build_freq_stratification_list,
    compute_freq_by_strata,
    faf_expr,
    gen_anc_faf_max_expr,
    generate_freq_group_membership_array,
    get_adj_expr,
    grpmax_expr,
    merge_freq_arrays,
    merge_histograms,
    qual_hist_expr,
    set_xx_y_metrics_to_na_expr,
)
from gnomad.utils.filtering import filter_arrays_by_meta
from gnomad.utils.release import make_freq_index_dict_from_meta
from gnomad.utils.slack import slack_notifications
from gnomad.utils.vcf import SORT_ORDER

from gnomad_qc.resource_utils import (
    PipelineResourceCollection,
    PipelineStepResourceCollection,
)
from gnomad_qc.slack_creds import slack_token
from gnomad_qc.v5.resources.basics import get_logging_path
from gnomad_qc.v5.resources.constants import WORKSPACE_BUCKET
from gnomad_qc.v5.resources.meta import project_meta
from gnomad_qc.v5.resources.sample_qc import (
    get_joint_qc,
    hard_filtered_samples,
    related_samples_to_drop,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s: %(message)s",
    datefmt="%m/%d/%Y %I:%M:%S %p",
)
logger = logging.getLogger("gnomAD_v5_frequency")
logger.setLevel(logging.INFO)

AGE_HISTS = [
    "age_hist_het",
    "age_hist_hom",
]
"""Age histograms to compute and keep on the frequency Table."""

QUAL_HISTS = [
    "gq_hist_all",
    "dp_hist_all",
    "gq_hist_alt",
    "dp_hist_alt",
    "ab_hist_alt",
]
"""Quality histograms to compute and keep on the frequency Table."""

FREQ_ROW_FIELDS = [
    "freq",
    "qual_hists",
    "raw_qual_hists",
    "age_hists",
]
"""
List of top level row and global annotations that we want on the frequency HT.
"""

FREQ_GLOBAL_FIELDS = [
    "downsamplings",
    "freq_meta",
    "age_distribution",
    "freq_index_dict",
    "freq_meta_sample_count",
]
"""
List of final global annotations created from dense data that we want on the frequency HT.
"""


def get_freq_resources(
    overwrite: bool = False,
    test: Optional[bool] = False,
    data_set: str = "gnomad",
) -> PipelineResourceCollection:
    """
    Get frequency resources for v5.

    :param overwrite: Whether to overwrite existing files.
    :param test: Whether to use test resources.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency resources.
    """
    freq_pipeline = PipelineResourceCollection(
        pipeline_name="v5_frequency",
        overwrite=overwrite,
    )

    # Define output resources for frequency calculations
    freq_output_resources = {
        "freq_ht": get_freq_ht(test=test, data_set=data_set),
        "age_hist_ht": get_age_hist_ht(test=test, data_set=data_set),
    }

    run_freq_calculations = PipelineStepResourceCollection(
        "--run-frequency-calculations",
        output_resources=freq_output_resources,
    )

    freq_pipeline.add_steps(
        {
            "run_freq_calculations": run_freq_calculations,
        }
    )

    return freq_pipeline


def get_freq_ht(test: bool = False, data_set: str = "gnomad") -> TableResource:
    """
    Get the frequency Table resource for v5.

    :param test: Whether to use test resources.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency Table resource.
    """
    if test:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/testing/frequency/{data_set}_freq_test.ht"
        )
    else:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/frequency/{data_set}_freq.ht"
        )


def get_age_hist_ht(test: bool = False, data_set: str = "gnomad") -> TableResource:
    """
    Get the age histogram Table resource for v5.

    :param test: Whether to use test resources.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Age histogram Table resource.
    """
    if test:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/testing/frequency/{data_set}_age_hist_test.ht"
        )
    else:
        return TableResource(
            f"gs://{WORKSPACE_BUCKET}/v5.0/frequency/{data_set}_age_hist.ht"
        )


def identify_samples_to_remove(
    meta_ht: hl.Table,
    relatedness_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Identify samples that will be removed from v5 due to relatedness or ancestry changes.

    :param meta_ht: Metadata Table with sample information.
    :param relatedness_ht: Relatedness Table with samples to drop.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Table with samples to remove and their reasons.
    """
    logger.info(f"Identifying samples to remove for {data_set} dataset...")

    # Get samples to drop due to relatedness
    samples_to_drop = related_samples_to_drop(test=False).ht()

    # Join with metadata to get ancestry information
    meta_with_drops = meta_ht.annotate(
        will_be_dropped=samples_to_drop[meta_ht.key].s.is_defined()
    )

    # For gnomAD, we need to compare v4 vs v5 ancestry assignments
    if data_set == "gnomad":
        # Load v4 metadata to compare ancestry assignments
        # TODO: Add v4 metadata loading once available
        # v4_meta_ht = get_v4_metadata().ht()
        # meta_with_drops = meta_with_drops.annotate(
        #     ancestry_changed=hl.if_else(
        #         v4_meta_ht[meta_with_drops.key].pop != meta_with_drops.pop,
        #         True,
        #         False
        #     )
        # )
        pass

    # For All of Us, we need to identify samples that will be filtered out
    elif data_set == "aou":
        # Identify AoU samples that will be filtered based on QC criteria
        # TODO: Add specific AoU filtering criteria
        # meta_with_drops = meta_with_drops.annotate(
        #     will_be_filtered=hl.if_else(
        #         meta_with_drops.qc_metrics.fail_any,
        #         True,
        #         False
        #     )
        # )
        pass

    return meta_with_drops


def modify_v4_frequency_for_removed_samples(
    v4_freq_ht: hl.Table,
    samples_to_remove: hl.Table,
    coverage_ht: Optional[hl.Table] = None,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Modify v4 frequency HT call statistics for samples that will be removed.

    :param v4_freq_ht: v4 frequency HT as base.
    :param samples_to_remove: Table with samples to remove.
    :param coverage_ht: Optional coverage Table with AN data.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Modified frequency Table with updated call statistics.
    """
    logger.info(f"Modifying v4 frequency for removed samples in {data_set} dataset...")

    # Get samples that will be removed
    removed_samples = samples_to_remove.filter(samples_to_remove.will_be_dropped)

    # Calculate frequency statistics for removed samples only
    # This will be used to subtract from the v4 frequency HT
    removed_freq_stats = calculate_removed_samples_frequency_stats(
        removed_samples, coverage_ht, data_set
    )

    # Subtract the removed samples' call statistics from v4 frequency HT
    modified_freq_ht = subtract_frequency_stats(v4_freq_ht, removed_freq_stats)

    return modified_freq_ht


def calculate_removed_samples_frequency_stats(
    removed_samples: hl.Table,
    coverage_ht: Optional[hl.Table] = None,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Calculate frequency statistics for samples that will be removed.

    :param removed_samples: Table with samples to remove.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :param coverage_ht: Optional coverage Table with AN data.
    :return: Frequency statistics for removed samples.
    """
    logger.info(
        f"Calculating frequency stats for removed samples in {data_set} dataset..."
    )

    # Load the VDS for the dataset
    if data_set == "gnomad":
        # Load gnomAD v4 VDS
        # TODO: Replace with appropriate resource once available
        # vds = get_gnomad_v4_vds().vds()
        logger.warning("gnomAD v4 VDS not available yet, using placeholder")
        return hl.Table.parallelize([])
    else:
        # Load All of Us VDS
        # TODO: Replace with appropriate resource once available
        # vds = get_aou_vds().vds()
        logger.warning("All of Us VDS not available yet, using placeholder")
        return hl.Table.parallelize([])

    # Filter VDS to only removed samples
    # This follows the v4 genome approach of filtering samples from VDS
    removed_sample_ids = removed_samples.filter(
        removed_samples.will_be_dropped
    ).s.collect()

    # Filter VDS to only include removed samples
    vds_filtered = hl.vds.filter_samples(vds, removed_sample_ids, keep=True)

    # Densify the filtered VDS for frequency calculation
    mt = hl.vds.to_dense_mt(vds_filtered)

    # Build frequency stratification
    strata_expr = build_freq_stratification_list(
        sex_expr=mt_removed.meta.sex_karyotype,
        gen_anc_expr=mt_removed.meta.pop,
        additional_strata_expr=[
            {"data_set": data_set},
            {"data_set": data_set, "pop": mt_removed.meta.pop},
        ],
    )

    # Generate frequency group membership
    group_membership_ht = generate_freq_group_membership_array(
        mt_removed.cols(),
        strata_expr,
        downsamplings=hl.literal(DOWNSAMPLINGS),
    )

    # Compute frequency statistics for removed samples
    freq_stats = compute_freq_by_strata(
        mt_removed,
        group_membership_ht[mt_removed.col_key].group_membership,
        strata_expr,
    )

    return freq_stats


def subtract_frequency_stats(
    base_freq_ht: hl.Table,
    removed_freq_stats: hl.Table,
) -> hl.Table:
    """
    Subtract frequency statistics of removed samples from base frequency HT.

    :param base_freq_ht: Base frequency HT (v4).
    :param removed_freq_stats: Frequency statistics for removed samples.
    :return: Modified frequency HT with removed samples' stats subtracted.
    """
    logger.info(
        "Subtracting removed samples' frequency statistics from base frequency HT..."
    )

    # Join the removed frequency stats with the base frequency HT
    joined_ht = base_freq_ht.annotate(
        removed_freq=removed_freq_stats[base_freq_ht.key].freq
    )

    # Subtract the removed samples' call statistics
    # This uses gnomAD's merge_freq_arrays with operation="diff"
    from gnomad.utils.annotations import merge_freq_arrays

    # Get the frequency arrays and metadata
    base_freq = joined_ht.freq
    removed_freq = joined_ht.removed_freq

    # Merge with diff operation to subtract removed samples' stats
    merged_freq, merged_meta, sample_counts = merge_freq_arrays(
        [base_freq, removed_freq],
        [base_freq_ht.freq_meta, removed_freq_stats.freq_meta],
        operation="diff",
        count_arrays={
            "counts": [
                base_freq_ht.freq_meta_sample_count,
                removed_freq_stats.freq_meta_sample_count,
            ]
        },
    )

    # Update the frequency HT with the merged results
    modified_ht = joined_ht.select(freq=merged_freq).select_globals(
        freq_meta=merged_meta,
        freq_meta_sample_count=sample_counts["counts"],
        freq_index_dict=make_freq_index_dict_from_meta(hl.literal(merged_meta)),
    )

    return modified_ht


def calculate_age_histograms_for_removed_samples(
    v4_freq_ht: hl.Table,
    samples_to_remove: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Calculate age histograms for samples that will be removed.

    :param v4_freq_ht: v4 frequency HT as base.
    :param samples_to_remove: Table with samples to remove.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Age histogram Table for removed samples.
    """
    logger.info(
        f"Calculating age histograms for removed samples in {data_set} dataset..."
    )

    # Get samples that will be removed
    removed_samples = samples_to_remove.filter(samples_to_remove.will_be_dropped)

    # Load the VDS for the dataset
    if data_set == "gnomad":
        # Load gnomAD v4 VDS
        # TODO: Replace with appropriate resource once available
        # vds = get_gnomad_v4_vds().vds()
        logger.warning("gnomAD v4 VDS not available yet, using placeholder")
        return hl.Table.parallelize([])
    else:
        # Load All of Us VDS
        # TODO: Replace with appropriate resource once available
        # vds = get_aou_vds().vds()
        logger.warning("All of Us VDS not available yet, using placeholder")
        return hl.Table.parallelize([])

    # Filter VDS to only removed samples
    # This follows the v4 genome approach of filtering samples from VDS
    removed_sample_ids = removed_samples.filter(
        removed_samples.will_be_dropped
    ).s.collect()

    # Filter VDS to only include removed samples
    vds_filtered = hl.vds.filter_samples(vds, removed_sample_ids, keep=True)

    # Densify the filtered VDS for age histogram calculation
    mt_removed = hl.vds.to_dense_mt(vds_filtered)

    # Calculate age histograms
    age_hist_expr = age_hists_expr(
        mt_removed.GT,
        mt_removed.meta.age,
        mt_removed.meta.sex_karyotype,
    )

    ht = mt_removed.aggregate_rows(
        age_hist_het=hl.agg.explode(age_hist_expr.age_hist_het),
        age_hist_hom=hl.agg.explode(age_hist_expr.age_hist_hom),
    )

    return ht


def integrate_coverage_an_data(
    freq_ht: hl.Table,
    coverage_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Integrate AN (allele number) data from coverage computation.

    :param freq_ht: Frequency Table.
    :param coverage_ht: Coverage Table with AN data.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency Table with integrated AN data.
    """
    logger.info(f"Integrating coverage AN data for {data_set} dataset...")

    # Join coverage AN data with frequency data
    freq_with_an = freq_ht.annotate(coverage_an=coverage_ht[freq_ht.key].AN)

    # Recalculate AF based on coverage AN if available
    freq_with_an = freq_with_an.annotate(
        freq=freq_with_an.freq.map(
            lambda x: x.annotate(
                AF=hl.if_else(
                    freq_with_an.coverage_an > 0,
                    x.AC / freq_with_an.coverage_an,
                    hl.missing(hl.tfloat64),
                )
            )
        )
    )

    return freq_with_an


def create_differential_frequency_summary(
    freq_ht: hl.Table,
    age_hist_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Create a summary of differential frequency changes.

    :param freq_ht: Frequency Table.
    :param age_hist_ht: Age histogram Table.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Summary Table with differential frequency information.
    """
    logger.info(f"Creating differential frequency summary for {data_set} dataset...")

    # Calculate summary statistics
    summary_ht = freq_ht.annotate(
        total_ac=hl.sum(freq_ht.freq.map(lambda x: x.AC)),
        total_an=hl.sum(freq_ht.freq.map(lambda x: x.AN)),
        avg_af=hl.sum(freq_ht.freq.map(lambda x: x.AF)) / hl.len(freq_ht.freq),
        total_hom=hl.sum(freq_ht.freq.map(lambda x: x.homozygote_count)),
    )

    # Add age histogram summary
    if age_hist_ht.count() > 0:
        summary_ht = summary_ht.annotate(age_distribution=age_hist_ht[summary_ht.key])

    return summary_ht


def handle_ancestry_changes(
    mt: hl.MatrixTable,
    samples_to_remove: hl.Table,
    v4_meta_ht: Optional[hl.Table] = None,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Handle samples that have changed ancestry between v4 and v5.

    :param mt: Input MatrixTable.
    :param samples_to_remove: Table with samples to remove.
    :param v4_meta_ht: Optional v4 metadata Table for ancestry comparison.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency Table for samples with ancestry changes.
    """
    logger.info(f"Handling ancestry changes for {data_set} dataset...")

    if v4_meta_ht is None:
        logger.warning("No v4 metadata provided, skipping ancestry change detection")
        return hl.Table.parallelize([])

    # Identify samples with ancestry changes
    samples_with_changes = samples_to_remove.annotate(
        ancestry_changed=hl.if_else(
            v4_meta_ht[samples_to_remove.key].pop != samples_to_remove.pop, True, False
        )
    ).filter(samples_with_changes.ancestry_changed)

    if samples_with_changes.count() == 0:
        logger.info("No samples with ancestry changes found")
        return hl.Table.parallelize([])

    # Calculate frequency for samples with ancestry changes
    mt_changed = mt.filter_cols(samples_with_changes[mt.col_key].s.is_defined())

    # Build frequency stratification for ancestry-changed samples
    strata_expr = build_freq_stratification_list(
        sex_expr=mt_changed.meta.sex_karyotype,
        gen_anc_expr=mt_changed.meta.pop,
        additional_strata_expr=[
            {"data_set": data_set, "ancestry_changed": True},
            {
                "data_set": data_set,
                "pop": mt_changed.meta.pop,
                "ancestry_changed": True,
            },
        ],
    )

    # Generate frequency group membership
    group_membership_ht = generate_freq_group_membership_array(
        mt_changed.cols(),
        strata_expr,
        downsamplings=hl.literal(DOWNSAMPLINGS),
    )

    # Compute frequency by strata
    freq_ht = compute_freq_by_strata(
        mt_changed,
        group_membership_ht[mt_changed.col_key].group_membership,
        strata_expr,
    )

    return freq_ht


def merge_frequency_data(
    gnomad_freq_ht: hl.Table,
    aou_freq_ht: hl.Table,
    test: bool = False,
) -> hl.Table:
    """
    Merge frequency data from gnomAD and All of Us datasets.

    :param gnomad_freq_ht: Frequency Table for gnomAD removed samples.
    :param aou_freq_ht: Frequency Table for AoU removed samples.
    :param test: Whether to use test resources.
    :return: Merged frequency Table.
    """
    logger.info("Merging frequency data from gnomAD and All of Us datasets...")

    # Combine frequency arrays
    freq_arrays = [gnomad_freq_ht.freq, aou_freq_ht.freq]
    freq_meta = [gnomad_freq_ht.freq_meta, aou_freq_ht.freq_meta]

    merged_freq, merged_freq_meta = merge_freq_arrays(
        freq_arrays,
        freq_meta,
    )

    # Merge histograms
    hist_structs = {
        "qual_hists": QUAL_HISTS,
        "raw_qual_hists": QUAL_HISTS,
        "age_hists": AGE_HISTS,
    }

    hists_expr = {
        hist_struct: hl.struct(
            **{
                h: merge_histograms(
                    [gnomad_freq_ht[hist_struct][h], aou_freq_ht[hist_struct][h]]
                )
                for h in hists
            }
        )
        for hist_struct, hists in hist_structs.items()
    }

    # Create merged Table
    merged_ht = gnomad_freq_ht.select(
        freq=merged_freq,
        **hists_expr,
    )

    merged_ht = merged_ht.annotate_globals(
        freq_meta=merged_freq_meta,
        freq_index_dict=make_freq_index_dict_from_meta(merged_freq_meta),
    )

    return merged_ht


def main(args):
    """Generate frequency data and age histograms for v5."""
    hl.init(
        log="/v5_frequency.log",
        default_reference="GRCh38",
        tmp_dir=f"gs://{WORKSPACE_BUCKET}/tmp/4_day",
    )

    # SSA Logs are easier to troubleshoot with.
    hl._set_flags(use_ssa_logs="1")

    test = args.test
    overwrite = args.overwrite
    data_set = args.data_set

    try:
        if args.run_frequency_calculations:
            logger.info(f"Running frequency calculations for {data_set} dataset...")

            # Load metadata and relatedness data
            meta_ht = project_meta.ht()
            relatedness_ht = related_samples_to_drop(test=test).ht()

            # Identify samples to remove
            samples_to_remove = identify_samples_to_remove(
                meta_ht, relatedness_ht, data_set
            )

            # Load v4 frequency HT as the base
            from gnomad_qc.v4.resources.annotations import get_freq

            v4_freq_ht = get_freq(data_type="exomes").ht()

            if test:
                logger.info(
                    "Filtering v4 freq HT to the first 2 partitions for testing..."
                )
                v4_freq_ht = v4_freq_ht._filter_partitions(range(2))

            # Load coverage data if available
            coverage_ht = None
            try:
                # TODO: Add coverage resource loading once available
                # coverage_ht = get_coverage_ht(test=test).ht()
                pass
            except Exception as e:
                logger.warning(f"Could not load coverage data: {e}")

            # Modify v4 frequency HT for removed samples
            freq_ht = modify_v4_frequency_for_removed_samples(
                v4_freq_ht, samples_to_remove, coverage_ht, data_set
            )

            # Integrate coverage AN data if available
            if coverage_ht is not None:
                freq_ht = integrate_coverage_an_data(freq_ht, coverage_ht, data_set)

            # Handle ancestry changes if v4 metadata is available
            v4_meta_ht = None
            try:
                # TODO: Add v4 metadata loading once available
                # v4_meta_ht = get_v4_metadata().ht()
                pass
            except Exception as e:
                logger.warning(f"Could not load v4 metadata: {e}")

            ancestry_changes_ht = handle_ancestry_changes(
                joint_qc_mt, samples_to_remove, v4_meta_ht, data_set
            )

            # Calculate age histograms for removed samples
            age_hist_ht = calculate_age_histograms_for_removed_samples(
                v4_freq_ht, samples_to_remove, data_set
            )

            # Create differential frequency summary if requested
            if args.generate_summary:
                summary_ht = create_differential_frequency_summary(
                    freq_ht, age_hist_ht, data_set
                )
                summary_ht.write(
                    get_freq_ht(test=test, data_set=f"{data_set}_summary").path,
                    overwrite=overwrite,
                )

            # Write results
            freq_ht.write(
                get_freq_ht(test=test, data_set=data_set).path, overwrite=overwrite
            )
            age_hist_ht.write(
                get_age_hist_ht(test=test, data_set=data_set).path, overwrite=overwrite
            )

            # Write ancestry changes results if any
            if ancestry_changes_ht.count() > 0:
                ancestry_changes_ht.write(
                    get_freq_ht(
                        test=test, data_set=f"{data_set}_ancestry_changes"
                    ).path,
                    overwrite=overwrite,
                )

        if args.merge_datasets:
            logger.info("Merging frequency data from both datasets...")

            # Load frequency data from both datasets
            gnomad_freq_ht = get_freq_ht(test=test, data_set="gnomad").ht()
            aou_freq_ht = get_freq_ht(test=test, data_set="aou").ht()

            # Merge frequency data
            merged_freq_ht = merge_frequency_data(
                gnomad_freq_ht, aou_freq_ht, test=test
            )

            # Write merged results
            merged_freq_ht.write(
                get_freq_ht(test=test, data_set="joint").path, overwrite=overwrite
            )

    finally:
        hl.copy_log(get_logging_path("v5_frequency"))


def get_script_argument_parser() -> argparse.ArgumentParser:
    """Get script argument parser."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--overwrite", help="Overwrite existing hail Tables.", action="store_true"
    )
    parser.add_argument(
        "--test",
        help="Filter to the first 2 partitions for testing.",
        action="store_true",
    )
    parser.add_argument(
        "--data-set",
        help="Dataset to process. One of 'gnomad', 'aou', or 'joint'.",
        type=str,
        choices=["gnomad", "aou", "joint"],
        default="gnomad",
    )
    parser.add_argument(
        "--run-frequency-calculations",
        help="Run frequency calculations for the specified dataset.",
        action="store_true",
    )
    parser.add_argument(
        "--merge-datasets",
        help="Merge frequency data from gnomAD and All of Us datasets.",
        action="store_true",
    )
    parser.add_argument(
        "--generate-summary",
        help="Generate differential frequency summary statistics.",
        action="store_true",
    )

    return parser


if __name__ == "__main__":
    parser = get_script_argument_parser()
    args = parser.parse_args()
    main(args)
