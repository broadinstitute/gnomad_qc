"""
Script to generate frequency data and age histograms for gnomAD v5.

This script calculates variant frequencies and generates age histograms for both gnomAD v4
samples that will be removed in v5 and All of Us samples that will be removed. It accounts
for samples that change ancestry between versions and handles the differential analysis
approach to avoid densifying the full dataset.

The script uses the AN (allele number) data generated by the coverage computation script
and accounts for samples that will be dropped due to relatedness filtering or ancestry changes.
"""

import argparse
import logging
from typing import Dict, List, Optional, Tuple, Union

import hail as hl
from gnomad.resources.grch38.gnomad import (
    DOWNSAMPLINGS,
    GEN_ANC_GROUPS_TO_REMOVE_FOR_GRPMAX,
)
from gnomad.resources.resource_utils import TableResource
from gnomad.sample_qc.sex import adjusted_sex_ploidy_expr
from gnomad.utils.annotations import (
    age_hists_expr,
    annotate_downsamplings,
    bi_allelic_site_inbreeding_expr,
    build_freq_stratification_list,
    compute_freq_by_strata,
    faf_expr,
    gen_anc_faf_max_expr,
    generate_freq_group_membership_array,
    get_adj_expr,
    grpmax_expr,
    merge_freq_arrays,
    merge_histograms,
    qual_hist_expr,
    set_xx_y_metrics_to_na_expr,
)
from gnomad.utils.filtering import filter_arrays_by_meta
from gnomad.utils.release import make_freq_index_dict_from_meta
from gnomad.utils.slack import slack_notifications
from gnomad.utils.vcf import SORT_ORDER

from gnomad_qc.resource_utils import (
    PipelineResourceCollection,
    PipelineStepResourceCollection,
)
from gnomad_qc.slack_creds import slack_token
from gnomad_qc.v4.resources.annotations import (
    get_all_sites_an_and_qual_hists as get_v4_coverage,
)
from gnomad_qc.v4.resources.annotations import get_freq as get_v4_freq

# Import v4 resources for metadata and frequency data
from gnomad_qc.v4.resources.meta import meta as v4_meta

# Import v5 annotation resources
from gnomad_qc.v5.resources.annotations import (
    get_age_hist_ht,
    get_ancestry_changes_ht,
    get_freq_ht,
    get_freq_summary_ht,
)
from gnomad_qc.v5.resources.basics import get_logging_path
from gnomad_qc.v5.resources.constants import WORKSPACE_BUCKET
from gnomad_qc.v5.resources.meta import project_meta
from gnomad_qc.v5.resources.sample_qc import (
    get_joint_qc,
    hard_filtered_samples,
    related_samples_to_drop,
)

logging.basicConfig(format="%(levelname)s (%(name)s %(lineno)s): %(message)s")
logger = logging.getLogger("v5_frequency")
logger.setLevel(logging.INFO)

AGE_HISTS = [
    "age_hist_het",
    "age_hist_hom",
]
"""Age histograms to compute and keep on the frequency Table."""

QUAL_HISTS = [
    "gq_hist_all",
    "dp_hist_all",
    "gq_hist_alt",
    "dp_hist_alt",
    "ab_hist_alt",
]
"""Quality histograms to compute and keep on the frequency Table."""

FREQ_ROW_FIELDS = [
    "freq",
    "qual_hists",
    "raw_qual_hists",
    "age_hists",
]
"""
List of top level row and global annotations that we want on the frequency HT.
"""

FREQ_GLOBAL_FIELDS = [
    "downsamplings",
    "freq_meta",
    "age_distribution",
    "freq_index_dict",
    "freq_meta_sample_count",
]
"""
List of final global annotations created from dense data that we want on the frequency HT.
"""


def get_freq_resources(
    overwrite: bool = False,
    test: Optional[bool] = False,
    data_set: str = "gnomad",
) -> PipelineResourceCollection:
    """
    Get frequency calculation resources.

    :param overwrite: Whether to overwrite existing results.
    :param test: Whether to run in test mode.
    :param data_set: Dataset identifier ("gnomad", "aou", "joint").
    :return: PipelineResourceCollection for frequency calculations.
    """
    freq_pipeline = PipelineResourceCollection(
        "v5_frequency",
        overwrite=overwrite,
        test=test,
    )

    # Add frequency calculation step
    freq_step = PipelineStepResourceCollection(
        "--run-frequency-calculations",
        input_resources={
            "v4_freq_ht": get_v4_freq(data_type="exomes"),
            "v4_meta_ht": v4_meta(data_type="exomes"),
            "v4_coverage_ht": get_v4_coverage(data_type="exomes", test=test),
            "project_meta": project_meta,
            "related_samples_to_drop": related_samples_to_drop(test=test),
        },
        output_resources={
            "freq_ht": get_freq_ht(test=test, data_set=data_set),
            "age_hist_ht": get_age_hist_ht(test=test, data_set=data_set),
            "freq_summary_ht": get_freq_summary_ht(test=test, data_set=data_set),
            "ancestry_changes_ht": get_ancestry_changes_ht(
                test=test, data_set=data_set
            ),
        },
    )

    freq_pipeline.add_steps({"freq_calculation": freq_step})

    return freq_pipeline


def get_aou_resources(
    overwrite: bool = False,
    test: Optional[bool] = False,
) -> PipelineResourceCollection:
    """
    Get All of Us dataset processing resources.

    :param overwrite: Whether to overwrite existing results.
    :param test: Whether to run in test mode.
    :return: PipelineResourceCollection for AoU processing.
    """
    aou_pipeline = PipelineResourceCollection(
        "v5_aou_frequency",
        overwrite=overwrite,
        test=test,
    )

    # Add AoU processing step
    aou_step = PipelineStepResourceCollection(
        "--process-aou-dataset",
        input_resources={
            "project_meta": project_meta,
            "related_samples_to_drop": related_samples_to_drop(test=test),
        },
        output_resources={
            "aou_freq_ht": get_freq_ht(test=test, data_set="aou"),
            "aou_age_hist_ht": get_age_hist_ht(test=test, data_set="aou"),
        },
    )

    aou_pipeline.add_steps({"aou_processing": aou_step})

    return aou_pipeline


def identify_samples_to_remove(
    meta_ht: hl.Table,
    relatedness_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Identify samples that will be removed from v5 due to relatedness or ancestry changes.

    :param meta_ht: Metadata Table with sample information.
    :param relatedness_ht: Relatedness Table with samples to drop.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Table with samples to remove and their reasons.
    """
    logger.info(f"Identifying samples to remove for {data_set} dataset...")

    # Get samples to drop due to relatedness
    samples_to_drop = related_samples_to_drop(test=False).ht()

    # Join with metadata to get ancestry information
    meta_with_drops = meta_ht.annotate(
        will_be_dropped=samples_to_drop[meta_ht.key].s.is_defined()
    )

    # For gnomAD, we need to compare v4 vs v5 ancestry assignments
    if data_set == "gnomad":
        # Load v4 metadata to compare ancestry assignments
        v4_meta_ht = v4_meta(data_type="exomes").ht()
        meta_with_drops = meta_with_drops.annotate(
            ancestry_changed=hl.if_else(
                v4_meta_ht[meta_with_drops.key].pop != meta_with_drops.pop, True, False
            )
        )

    # For All of Us, we need to identify samples that will be filtered out
    elif data_set == "aou":
        # Identify AoU samples that will be filtered based on QC criteria
        meta_with_drops = meta_with_drops.annotate(
            will_be_filtered=hl.if_else(
                meta_with_drops.qc_metrics.fail_any, True, False
            )
        )

    return meta_with_drops


def modify_v4_frequency_for_removed_samples(
    v4_freq_ht: hl.Table,
    samples_to_remove: hl.Table,
    coverage_ht: Optional[hl.Table] = None,
    data_set: str = "gnomad",
    test: bool = False,
) -> hl.Table:
    """
    Modify v4 frequency HT call statistics for samples that will be removed.

    :param v4_freq_ht: v4 frequency HT as base.
    :param samples_to_remove: Table with samples to remove.
    :param coverage_ht: Optional coverage Table with AN data.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Modified frequency Table with updated call statistics.
    """
    logger.info(f"Modifying v4 frequency for removed samples in {data_set} dataset...")

    # Get samples that will be removed
    removed_samples = samples_to_remove.filter(samples_to_remove.will_be_dropped)

    # Calculate frequency statistics for removed samples only
    # This will be used to subtract from the v4 frequency HT
    removed_freq_stats = calculate_removed_samples_frequency_stats(
        removed_samples, coverage_ht, data_set, test
    )

    # Subtract the removed samples' call statistics from v4 frequency HT
    modified_freq_ht = subtract_frequency_stats(v4_freq_ht, removed_freq_stats)

    return modified_freq_ht


def calculate_removed_samples_frequency_stats(
    removed_samples: hl.Table,
    coverage_ht: Optional[hl.Table] = None,
    data_set: str = "gnomad",
    test: bool = False,
) -> hl.Table:
    """
    Calculate frequency statistics for samples that will be removed.

    :param removed_samples: Table with samples to remove.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :param coverage_ht: Optional coverage Table with AN data.
    :return: Frequency statistics for removed samples.
    """
    logger.info(
        f"Calculating frequency stats for removed samples in {data_set} dataset..."
    )

    # Load the VDS for the dataset
    if data_set == "gnomad":
        # Load gnomAD v4 VDS
        from gnomad_qc.v4.resources.basics import get_gnomad_v4_vds

        vds = get_gnomad_v4_vds(release_only=True, annotate_meta=True, test=test)
    else:
        # Load All of Us VDS
        from gnomad_qc.v5.resources.basics import get_aou_vds

        vds = get_aou_vds(remove_hard_filtered_samples=True, test=test)

    # Filter VDS to only removed samples
    # This follows the v4 genome approach of filtering samples from VDS
    removed_sample_ids = removed_samples.filter(
        removed_samples.will_be_dropped
    ).s.collect()

    # Filter VDS to only include removed samples
    vds_filtered = hl.vds.filter_samples(vds, removed_sample_ids, keep=True)

    # Densify the filtered VDS for frequency calculation
    mt = hl.vds.to_dense_mt(vds_filtered)

    # Build frequency stratification
    strata_expr = build_freq_stratification_list(
        sex_expr=mt.meta.sex_karyotype,
        gen_anc_expr=mt.meta.pop,
        additional_strata_expr=[
            {"data_set": data_set},
            {"data_set": data_set, "pop": mt.meta.pop},
        ],
    )

    # Generate frequency group membership
    group_membership_ht = generate_freq_group_membership_array(
        mt.cols(),
        strata_expr,
        downsamplings=hl.literal(DOWNSAMPLINGS),
    )

    # Compute frequency statistics
    freq_ht = compute_freq_by_strata(
        mt,
        group_membership_ht,
        coverage_ht=coverage_ht,
        test=test,
    )

    return freq_ht


def subtract_frequency_stats(
    base_freq_ht: hl.Table,
    removed_freq_stats: hl.Table,
) -> hl.Table:
    """
    Subtract frequency statistics of removed samples from base frequency HT.

    :param base_freq_ht: Base frequency HT (v4).
    :param removed_freq_stats: Frequency statistics for removed samples.
    :return: Modified frequency HT with removed samples' stats subtracted.
    """
    logger.info(
        "Subtracting removed samples' frequency statistics from base frequency HT..."
    )

    # Join the removed frequency stats with the base frequency HT
    joined_ht = base_freq_ht.annotate(
        removed_freq=removed_freq_stats[base_freq_ht.key].freq
    )

    # Subtract the removed samples' call statistics
    # This uses gnomAD's merge_freq_arrays with operation="diff"
    from gnomad.utils.annotations import merge_freq_arrays

    # Get the frequency arrays and metadata
    base_freq = joined_ht.freq
    removed_freq = joined_ht.removed_freq

    # Merge with diff operation to subtract removed samples' stats
    merged_freq, merged_meta, sample_counts = merge_freq_arrays(
        [base_freq, removed_freq],
        [base_freq_ht.freq_meta, removed_freq_stats.freq_meta],
        operation="diff",
        count_arrays={
            "counts": [
                base_freq_ht.freq_meta_sample_count,
                removed_freq_stats.freq_meta_sample_count,
            ]
        },
    )

    # Update the frequency HT with the merged results
    modified_ht = joined_ht.select(freq=merged_freq).select_globals(
        freq_meta=merged_meta,
        freq_meta_sample_count=sample_counts["counts"],
        freq_index_dict=make_freq_index_dict_from_meta(hl.literal(merged_meta)),
    )

    return modified_ht


def calculate_age_histograms_for_removed_samples(
    v4_freq_ht: hl.Table,
    samples_to_remove: hl.Table,
    data_set: str = "gnomad",
    test: bool = False,
) -> hl.Table:
    """
    Calculate age histograms for samples that will be removed.

    :param v4_freq_ht: v4 frequency HT as base.
    :param samples_to_remove: Table with samples to remove.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Age histogram Table for removed samples.
    """
    logger.info(
        f"Calculating age histograms for removed samples in {data_set} dataset..."
    )

    # Get samples that will be removed
    removed_samples = samples_to_remove.filter(samples_to_remove.will_be_dropped)

    # Load the VDS for the dataset
    if data_set == "gnomad":
        # Load gnomAD v4 VDS
        from gnomad_qc.v4.resources.basics import get_gnomad_v4_vds

        vds = get_gnomad_v4_vds(release_only=True, annotate_meta=True, test=test)
    else:
        # Load All of Us VDS
        from gnomad_qc.v5.resources.basics import get_aou_vds

        vds = get_aou_vds(remove_hard_filtered_samples=True, test=test)

    # Filter VDS to only removed samples
    # This follows the v4 genome approach of filtering samples from VDS
    removed_sample_ids = removed_samples.filter(
        removed_samples.will_be_dropped
    ).s.collect()

    # Filter VDS to only include removed samples
    vds_filtered = hl.vds.filter_samples(vds, removed_sample_ids, keep=True)

    # Densify the filtered VDS for age histogram calculation
    mt_removed = hl.vds.to_dense_mt(vds_filtered)

    # Calculate age histograms
    age_hist_expr = age_hists_expr(
        mt_removed.GT,
        mt_removed.meta.age,
        mt_removed.meta.sex_karyotype,
    )

    ht = mt_removed.aggregate_rows(
        age_hist_het=hl.agg.explode(age_hist_expr.age_hist_het),
        age_hist_hom=hl.agg.explode(age_hist_expr.age_hist_hom),
    )

    return ht


def integrate_coverage_an_data(
    freq_ht: hl.Table,
    coverage_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Integrate AN (allele number) data from coverage computation.

    :param freq_ht: Frequency Table.
    :param coverage_ht: Coverage Table with AN data.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Frequency Table with integrated AN data.
    """
    logger.info(f"Integrating coverage AN data for {data_set} dataset...")

    # Join coverage AN data with frequency data
    freq_with_an = freq_ht.annotate(coverage_an=coverage_ht[freq_ht.key].AN)

    # Recalculate AF based on coverage AN if available
    freq_with_an = freq_with_an.annotate(
        freq=freq_with_an.freq.map(
            lambda x: x.annotate(
                AF=hl.if_else(
                    freq_with_an.coverage_an > 0,
                    x.AC / freq_with_an.coverage_an,
                    hl.missing(hl.tfloat64),
                )
            )
        )
    )

    return freq_with_an


def create_differential_frequency_summary(
    freq_ht: hl.Table,
    age_hist_ht: hl.Table,
    data_set: str = "gnomad",
) -> hl.Table:
    """
    Create a summary of differential frequency changes.

    :param freq_ht: Frequency Table.
    :param age_hist_ht: Age histogram Table.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :return: Summary Table with differential frequency information.
    """
    logger.info(f"Creating differential frequency summary for {data_set} dataset...")

    # Calculate summary statistics
    summary_ht = freq_ht.annotate(
        total_ac=hl.sum(freq_ht.freq.map(lambda x: x.AC)),
        total_an=hl.sum(freq_ht.freq.map(lambda x: x.AN)),
        avg_af=hl.sum(freq_ht.freq.map(lambda x: x.AF)) / hl.len(freq_ht.freq),
        total_hom=hl.sum(freq_ht.freq.map(lambda x: x.homozygote_count)),
    )

    # Add age histogram summary
    if age_hist_ht.count() > 0:
        summary_ht = summary_ht.annotate(age_distribution=age_hist_ht[summary_ht.key])

    return summary_ht


def handle_ancestry_changes(
    v4_freq_ht: hl.Table,
    samples_to_remove: hl.Table,
    v4_meta_ht: Optional[hl.Table] = None,
    data_set: str = "gnomad",
    test: bool = False,
) -> hl.Table:
    """
    Handle samples that have changed ancestry between v4 and v5.

    :param v4_freq_ht: v4 frequency HT as base.
    :param samples_to_remove: Table with samples to remove.
    :param v4_meta_ht: v4 metadata Table for ancestry comparison.
    :param data_set: Dataset identifier ("gnomad" or "aou").
    :param test: Whether to run in test mode.
    :return: Frequency Table for samples with ancestry changes.
    """
    logger.info(f"Handling ancestry changes for {data_set} dataset...")

    if v4_meta_ht is None:
        logger.warning("No v4 metadata provided, skipping ancestry change detection")
        return hl.Table.parallelize([])

    # Load v5 metadata for comparison
    v5_meta_ht = project_meta.ht()

    # Identify samples with ancestry changes
    samples_with_changes = samples_to_remove.annotate(
        v4_pop=v4_meta_ht[samples_to_remove.key].pop,
        v5_pop=v5_meta_ht[samples_to_remove.key].pop,
        ancestry_changed=hl.if_else(
            v4_meta_ht[samples_to_remove.key].pop
            != v5_meta_ht[samples_to_remove.key].pop,
            True,
            False,
        ),
    ).filter(samples_with_changes.ancestry_changed)

    if samples_with_changes.count() == 0:
        logger.info("No samples with ancestry changes found")
        return hl.Table.parallelize([])

    # Load the VDS for the dataset
    if data_set == "gnomad":
        from gnomad_qc.v4.resources.basics import get_gnomad_v4_vds

        vds = get_gnomad_v4_vds(release_only=True, annotate_meta=True, test=test)
    else:
        from gnomad_qc.v5.resources.basics import get_aou_vds

        vds = get_aou_vds(remove_hard_filtered_samples=True, test=test)

    # Filter VDS to only samples with ancestry changes
    changed_sample_ids = samples_with_changes.s.collect()
    vds_filtered = hl.vds.filter_samples(vds, changed_sample_ids, keep=True)
    mt = hl.vds.to_dense_mt(vds_filtered)

    # Build frequency stratification for ancestry-changed samples
    strata_expr = build_freq_stratification_list(
        sex_expr=mt.meta.sex_karyotype,
        gen_anc_expr=mt.meta.pop,
        additional_strata_expr=[
            {"data_set": data_set, "ancestry_changed": True},
            {
                "data_set": data_set,
                "pop": mt.meta.pop,
                "ancestry_changed": True,
            },
        ],
    )

    # Generate frequency group membership
    group_membership_ht = generate_freq_group_membership_array(
        mt.cols(),
        strata_expr,
        downsamplings=hl.literal(DOWNSAMPLINGS),
    )

    # Compute frequency statistics for ancestry-changed samples
    freq_ht = compute_freq_by_strata(
        mt,
        group_membership_ht,
        test=test,
    )

    return freq_ht


def process_aou_dataset(
    test: bool = False,
    overwrite: bool = False,
) -> hl.Table:
    """
    Process All of Us dataset for frequency calculations.

    This function handles the AoU dataset specifically, using the sparse MT from the VDS
    to get samples we are removing, and processes AN for all sites and all samples.

    :param test: Whether to run in test mode.
    :param overwrite: Whether to overwrite existing results.
    :return: Frequency Table for AoU dataset.
    """
    logger.info("Processing All of Us dataset...")

    # Load AoU VDS
    from gnomad_qc.v5.resources.basics import get_aou_vds

    aou_vds = get_aou_vds(remove_hard_filtered_samples=True, test=test)

    # Load AoU metadata and relatedness data
    aou_meta_ht = project_meta.ht()
    aou_relatedness_ht = related_samples_to_drop(test=test).ht()

    # Identify AoU samples to remove
    aou_samples_to_remove = identify_samples_to_remove(
        aou_meta_ht, aou_relatedness_ht, "aou"
    )

    # Get samples that will be dropped from AoU
    aou_removed_sample_ids = aou_samples_to_remove.filter(
        aou_samples_to_remove.will_be_dropped
    ).s.collect()

    # Filter AoU VDS to only removed samples using sparse MT approach
    aou_vds_filtered = hl.vds.filter_samples(aou_vds, aou_removed_sample_ids, keep=True)

    # Densify the filtered AoU VDS for frequency calculation
    aou_mt = hl.vds.to_dense_mt(aou_vds_filtered)

    # Build frequency stratification for AoU
    aou_strata_expr = build_freq_stratification_list(
        sex_expr=aou_mt.meta.sex_karyotype,
        gen_anc_expr=aou_mt.meta.pop,
        additional_strata_expr=[
            {"data_set": "aou"},
            {"data_set": "aou", "pop": aou_mt.meta.pop},
        ],
    )

    # Generate frequency group membership for AoU
    aou_group_membership_ht = generate_freq_group_membership_array(
        aou_mt.cols(),
        aou_strata_expr,
        downsamplings=hl.literal(DOWNSAMPLINGS),
    )

    # Compute frequency statistics for AoU removed samples
    aou_freq_ht = compute_freq_by_strata(
        aou_mt,
        aou_group_membership_ht,
        test=test,
    )

    # Calculate age histograms for AoU removed samples
    aou_age_hist_expr = age_hists_expr(
        aou_mt.meta.age,
        aou_mt.meta.sex_karyotype,
        aou_mt.GT,
        aou_mt.adj,
    )

    aou_age_hist_ht = aou_mt.aggregate_rows(
        hl.struct(
            age_hist=hl.agg.group_by(
                hl.struct(
                    pop=aou_mt.meta.pop,
                    sex=aou_mt.meta.sex_karyotype,
                    data_set="aou",
                ),
                aou_age_hist_expr,
            )
        )
    )

    return aou_freq_ht, aou_age_hist_ht


def merge_frequency_data(
    gnomad_freq_ht: hl.Table,
    aou_freq_ht: hl.Table,
    test: bool = False,
) -> hl.Table:
    """
    Merge frequency data from gnomAD and All of Us datasets.

    :param gnomad_freq_ht: Frequency Table for gnomAD removed samples.
    :param aou_freq_ht: Frequency Table for AoU removed samples.
    :param test: Whether to use test resources.
    :return: Merged frequency Table.
    """
    logger.info("Merging frequency data from gnomAD and All of Us datasets...")

    # Combine frequency arrays
    freq_arrays = [gnomad_freq_ht.freq, aou_freq_ht.freq]
    freq_meta = [gnomad_freq_ht.freq_meta, aou_freq_ht.freq_meta]

    merged_freq, merged_freq_meta = merge_freq_arrays(
        freq_arrays,
        freq_meta,
    )

    # Merge histograms
    hist_structs = {
        "qual_hists": QUAL_HISTS,
        "raw_qual_hists": QUAL_HISTS,
        "age_hists": AGE_HISTS,
    }

    hists_expr = {
        hist_struct: hl.struct(
            **{
                h: merge_histograms(
                    [gnomad_freq_ht[hist_struct][h], aou_freq_ht[hist_struct][h]]
                )
                for h in hists
            }
        )
        for hist_struct, hists in hist_structs.items()
    }

    # Create merged Table
    merged_ht = gnomad_freq_ht.select(
        freq=merged_freq,
        **hists_expr,
    )

    merged_ht = merged_ht.annotate_globals(
        freq_meta=merged_freq_meta,
        freq_index_dict=make_freq_index_dict_from_meta(merged_freq_meta),
    )

    return merged_ht


def main(args):
    """Generate frequency data and age histograms for v5."""
    hl.init(
        log="/v5_frequency.log",
        default_reference="GRCh38",
        tmp_dir=f"gs://{WORKSPACE_BUCKET}/tmp/4_day",
    )

    # SSA Logs are easier to troubleshoot with.
    hl._set_flags(use_ssa_logs="1")

    test = args.test
    overwrite = args.overwrite
    data_set = args.data_set

    try:
        if args.run_frequency_calculations:
            logger.info(f"Running frequency calculations for {data_set} dataset...")

            # Load metadata and relatedness data
            meta_ht = project_meta.ht()
            relatedness_ht = related_samples_to_drop(test=test).ht()

            # Identify samples to remove
            samples_to_remove = identify_samples_to_remove(
                meta_ht, relatedness_ht, data_set
            )

            # Load v4 frequency HT as the base
            v4_freq_ht = get_v4_freq(data_type="exomes").ht()

            if test:
                logger.info(
                    "Filtering v4 freq HT to the first 2 partitions for testing..."
                )
                v4_freq_ht = v4_freq_ht._filter_partitions(range(2))

            # Load coverage data if available
            coverage_ht = None
            try:
                # Load coverage HT from v4 coverage computation
                coverage_ht = get_v4_coverage(data_type="exomes", test=test).ht()
                if test:
                    coverage_ht = coverage_ht._filter_partitions(range(2))
            except Exception as e:
                logger.warning(f"Could not load coverage data: {e}")

            # Modify v4 frequency HT for removed samples
            freq_ht = modify_v4_frequency_for_removed_samples(
                v4_freq_ht, samples_to_remove, coverage_ht, data_set, test
            )

            # Integrate coverage AN data if available
            if coverage_ht is not None:
                freq_ht = integrate_coverage_an_data(freq_ht, coverage_ht, data_set)

            # Load v4 metadata for ancestry comparison
            v4_meta_ht = v4_meta(data_type="exomes").ht()
            if test:
                v4_meta_ht = v4_meta_ht._filter_partitions(range(2))

            # Handle ancestry changes
            ancestry_changes_ht = handle_ancestry_changes(
                v4_freq_ht, samples_to_remove, v4_meta_ht, data_set, test
            )

            # Calculate age histograms for removed samples
            age_hist_ht = calculate_age_histograms_for_removed_samples(
                v4_freq_ht, samples_to_remove, data_set, test
            )

            # Create differential frequency summary if requested
            if args.generate_summary:
                summary_ht = create_differential_frequency_summary(
                    freq_ht, age_hist_ht, data_set
                )
                summary_ht.write(
                    get_freq_summary_ht(test=test, data_set=data_set).path,
                    overwrite=overwrite,
                )

            # Write results
            freq_ht.write(
                get_freq_ht(test=test, data_set=data_set).path, overwrite=overwrite
            )
            age_hist_ht.write(
                get_age_hist_ht(test=test, data_set=data_set).path, overwrite=overwrite
            )

            # Write ancestry changes results if any
            if ancestry_changes_ht.count() > 0:
                ancestry_changes_ht.write(
                    get_ancestry_changes_ht(test=test, data_set=data_set).path,
                    overwrite=overwrite,
                )

        if args.process_aou_dataset:
            logger.info("Processing All of Us dataset...")

            # Process AoU dataset
            aou_freq_ht, aou_age_hist_ht = process_aou_dataset(
                test=test, overwrite=overwrite
            )

            # Write AoU results
            aou_freq_ht.write(
                get_freq_ht(test=test, data_set="aou").path, overwrite=overwrite
            )
            aou_age_hist_ht.write(
                get_age_hist_ht(test=test, data_set="aou").path, overwrite=overwrite
            )

        if args.merge_datasets:
            logger.info("Merging frequency data from both datasets...")

            # Load frequency data from both datasets
            gnomad_freq_ht = get_freq_ht(test=test, data_set="gnomad").ht()
            aou_freq_ht = get_freq_ht(test=test, data_set="aou").ht()

            # Merge frequency data
            merged_freq_ht = merge_frequency_data(
                gnomad_freq_ht, aou_freq_ht, test=test
            )

            # Write merged results
            merged_freq_ht.write(
                get_freq_ht(test=test, data_set="joint").path, overwrite=overwrite
            )

    finally:
        hl.copy_log(get_logging_path("v5_frequency"))


def get_script_argument_parser() -> argparse.ArgumentParser:
    """Get script argument parser."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--overwrite", help="Overwrite existing hail Tables.", action="store_true"
    )
    parser.add_argument(
        "--test",
        help="Filter to the first 2 partitions for testing.",
        action="store_true",
    )
    parser.add_argument(
        "--data-set",
        help="Dataset to process. One of 'gnomad', 'aou', or 'joint'.",
        type=str,
        choices=["gnomad", "aou", "joint"],
        default="gnomad",
    )
    parser.add_argument(
        "--run-frequency-calculations",
        help="Run frequency calculations for the specified dataset.",
        action="store_true",
    )
    parser.add_argument(
        "--process-aou-dataset",
        help="Process All of Us dataset specifically.",
        action="store_true",
    )
    parser.add_argument(
        "--merge-datasets",
        help="Merge frequency data from both gnomAD and AoU datasets.",
        action="store_true",
    )
    parser.add_argument(
        "--generate-summary",
        help="Generate differential frequency summary.",
        action="store_true",
    )

    return parser


if __name__ == "__main__":
    parser = get_script_argument_parser()
    args = parser.parse_args()
    main(args)
