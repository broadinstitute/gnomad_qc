{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"font-family:'arial';font-size:25px\"> Set up for computing with cuKING on the *All of Us* Researcher Workbench with dsub </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this notebook, your cloud analysis environment can be left with the default settings for a General Analyses.\n",
    "- This notebook only takes a couple minutes to run interactively. \n",
    "- This notebook writes a cuKING_dsub.bash file containing the bash `cuKING_dsub` function to disk which is then sourced within the .bashrc so the function can be used when the terminal or the file can be sourced within a notebook cell to use it interactively.\n",
    "\n",
    "**This notebook only needs to be run once per compute environment. If the environment is paused the notebook does not need to be rerun. If it is deleted, the notebook will need to be rerun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Cloud Analysis Environment</b>: Use \"Recommended Environment\" <kbd><b>General Analysis</b></kbd> which creates compute type <kbd><b>Standard VM</b></kbd> with default values of 4 CPUs, 15GB RAM, and 120GB disk.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "This notebook will set up [dsub](https://github.com/databiosphere/dsub) with the cuKING image from the gnomAD artifact registry for use on the Researcher Workbench.\n",
    "\n",
    "See also the [dsub documentation](https://github.com/databiosphere/dsub#dsub-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is dsub?\n",
    "\n",
    "dsub is a command-line tool that makes it easy to submit and run batch scripts in the cloud.\n",
    "\n",
    "The dsub user experience is modeled after traditional high-performance computing job schedulers like Grid Engine and Slurm. You write a script and then submit it to a job scheduler from a shell prompt. You can submit via jupyter notebook cell or via the terminal application after running this notebook which will edit the .bashrc.\n",
    "\n",
    "NOTE: You need to source the .bashrc in every cell that you use the cuking_dsub command in. Each cell is independent so running source once, as you will in the terminal, will not work in the notebook.\n",
    "\n",
    "See also the [dsub documentation](https://github.com/databiosphere/dsub#dsub-features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use dsub?\n",
    "\n",
    "You can use `%%bash` or `!` in a notebook to run command line tools like `plink`. It works fine, and it's nice to show your work in a [literate programming style](https://en.wikipedia.org/wiki/Literate_programming)! You can of course also use the Jupyter terminal to run command line tools like `plink`.\n",
    "\n",
    "\n",
    "You might prefer to run those command line tools via scripts with dsub when you want to:\n",
    "* run the script in **parallel** (e.g., to process data for different chromosomes on different machines simultaneously)\n",
    "* run the script **on a different machine** than where Jupyter is running (e.g., so that CPU and RAM are dedicated, not shared)\n",
    "* run something that may take **longer than 24 hours** (e.g., to avoid cloud analysis environment [autopause](https://support.terra.bio/hc/en-us/articles/360029761352-Preventing-runaway-costs-with-notebook-auto-pause-#h_de5698f5-3c82-4763-aaaf-ea7df6a1869c))\n",
    "* run something using **inexpensive [preemptible VMs](https://cloud.google.com/compute/docs/instances/preemptible)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dsub in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: funcsigs==1.0.2 in /opt/conda/lib/python3.10/site-packages (from dsub) (1.0.2)\n",
      "Requirement already satisfied: google-api-core<=2.19.0,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.19.0)\n",
      "Requirement already satisfied: google-api-python-client<=2.131.0,>=2.47.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.131.0)\n",
      "Requirement already satisfied: google-auth-httplib2<=0.2.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.2.0)\n",
      "Requirement already satisfied: google-auth<=2.29.0,>=2.6.6 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.29.0)\n",
      "Requirement already satisfied: google-cloud-batch<=0.17.20 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.17.20)\n",
      "Requirement already satisfied: httplib2<=0.22.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.21.0)\n",
      "Requirement already satisfied: mock<=5.1.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (5.1.0)\n",
      "Requirement already satisfied: parameterized<=0.9.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.9.0)\n",
      "Requirement already satisfied: protobuf<=5.26.0,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (3.20.2)\n",
      "Requirement already satisfied: pyasn1-modules<=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.4.0)\n",
      "Requirement already satisfied: pyasn1<=0.6.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<=2.9.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (2.8.2)\n",
      "Requirement already satisfied: pytz<=2024.1 in /opt/conda/lib/python3.10/site-packages (from dsub) (2023.3)\n",
      "Requirement already satisfied: pyyaml<=6.0.1 in /opt/conda/lib/python3.10/site-packages (from dsub) (6.0.1)\n",
      "Requirement already satisfied: rsa<=4.9 in /opt/conda/lib/python3.10/site-packages (from dsub) (4.9)\n",
      "Requirement already satisfied: tabulate<=0.9.0 in /opt/conda/lib/python3.10/site-packages (from dsub) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=8.2.3 in /opt/conda/lib/python3.10/site-packages (from dsub) (8.2.3)\n",
      "Requirement already satisfied: uritemplate<=4.1.1 in /opt/conda/lib/python3.10/site-packages (from dsub) (3.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<=2.19.0,>=2.7.3->dsub) (1.66.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core<=2.19.0,>=2.7.3->dsub) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core<=2.19.0,>=2.7.3->dsub) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<=2.29.0,>=2.6.6->dsub) (4.2.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-batch<=0.17.20->dsub) (1.68.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-batch<=0.17.20->dsub) (1.48.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<=0.22.0->dsub) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<=2.9.0->dsub) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<=2.19.0,>=2.7.3->dsub) (2024.12.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# optional as dsub is installed already\n",
    "!pip3 install --upgrade dsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View `dsub --help`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dsub has many parameters. It is a really flexible system, but for several of the parameter values, we either always want to pass the same value when running on the *All of Us* Researcher Workbench, or there is a recommended default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: /opt/conda/bin/dsub [-h] [--provider PROVIDER] [--version VERSION]\n",
      "                           [--unique-job-id] [--name NAME]\n",
      "                           [--tasks [FILE M-N ...]] [--image IMAGE]\n",
      "                           [--dry-run] [--command COMMAND] [--script SCRIPT]\n",
      "                           [--env [KEY=VALUE ...]] [--label [KEY=VALUE ...]]\n",
      "                           [--input [KEY=REMOTE_PATH ...]]\n",
      "                           [--input-recursive [KEY=REMOTE_PATH ...]]\n",
      "                           [--output [KEY=REMOTE_PATH ...]]\n",
      "                           [--output-recursive [KEY=REMOTE_PATH ...]]\n",
      "                           [--user USER] [--user-project USER_PROJECT]\n",
      "                           [--mount [KEY=PATH_SPEC ...]] [--wait]\n",
      "                           [--retries RETRIES] [--poll-interval POLL_INTERVAL]\n",
      "                           [--after AFTER [AFTER ...]] [--skip] [--summary]\n",
      "                           [--min-cores MIN_CORES] [--min-ram MIN_RAM]\n",
      "                           [--disk-size DISK_SIZE] [--logging LOGGING]\n",
      "                           [--project PROJECT]\n",
      "                           [--boot-disk-size BOOT_DISK_SIZE]\n",
      "                           [--preemptible [PREEMPTIBLE]]\n",
      "                           [--zones ZONES [ZONES ...]]\n",
      "                           [--scopes SCOPES [SCOPES ...]]\n",
      "                           [--accelerator-type ACCELERATOR_TYPE]\n",
      "                           [--accelerator-count ACCELERATOR_COUNT]\n",
      "                           [--credentials-file CREDENTIALS_FILE]\n",
      "                           [--regions REGIONS [REGIONS ...]]\n",
      "                           [--machine-type MACHINE_TYPE]\n",
      "                           [--cpu-platform CPU_PLATFORM] [--network NETWORK]\n",
      "                           [--subnetwork SUBNETWORK] [--use-private-address]\n",
      "                           [--timeout TIMEOUT] [--log-interval LOG_INTERVAL]\n",
      "                           [--ssh] [--service-account SERVICE_ACCOUNT]\n",
      "                           [--disk-type DISK_TYPE]\n",
      "                           [--enable-stackdriver-monitoring]\n",
      "                           [--block-external-network] [--location LOCATION]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --provider PROVIDER   Job service provider. Valid values are \"google-cls-v2\"\n",
      "                        (Google's Pipelines API v2beta), \"google-batch\"\n",
      "                        (Google's Batch API v1alpha1), and \"local\" (local\n",
      "                        Docker execution). \"test-*\" providers are for testing\n",
      "                        purposes only. (default: google-cls-v2)\n",
      "  --version VERSION, -v VERSION\n",
      "                        Print the dsub version and exit.\n",
      "  --unique-job-id       Experimental: create a unique 32 character UUID for\n",
      "                        the dsub job-id using\n",
      "                        https://docs.python.org/3/library/uuid.html. (default:\n",
      "                        False)\n",
      "  --name NAME           Name for the job. Defaults to the script name or first\n",
      "                        token of the --command if specified.\n",
      "  --tasks [FILE M-N ...]\n",
      "                        Path to a file of tab separated values (TSV) for task\n",
      "                        parameters. The file may be located in the local\n",
      "                        filesystem or in a Google Cloud Storage bucket. The\n",
      "                        first line is a list of column headers specifying an\n",
      "                        --env, --input, --input-recursive, --output or\n",
      "                        --output-recursive variable, and each subsequent line\n",
      "                        specifies the values for a task. Optionally specify\n",
      "                        tasks from the file to submit. Can take the form \"m\",\n",
      "                        \"m-\", or \"m-n\" where m and n are task numbers starting\n",
      "                        at 1. (default: None)\n",
      "  --image IMAGE         Image name from Docker Hub, Google Container\n",
      "                        Repository, or other Docker image service. The task\n",
      "                        must have READ access to the image. (default:\n",
      "                        ubuntu:14.04)\n",
      "  --dry-run             Print the task(s) that would be run and then exit.\n",
      "                        (default: False)\n",
      "  --command COMMAND     Command to run inside the job's Docker container. This\n",
      "                        argument or the --script argument must be provided.\n",
      "  --script SCRIPT       Path to a script that is located in the local file\n",
      "                        system or inside a Google Cloud Storage bucket. This\n",
      "                        script will be run inside the job's Docker container.\n",
      "                        This argument or the --command argument must be\n",
      "                        provided.\n",
      "  --env [KEY=VALUE ...]\n",
      "                        Environment variables for the script's execution\n",
      "                        environment\n",
      "  --label [KEY=VALUE ...]\n",
      "                        Labels to associate to the job.\n",
      "  --input [KEY=REMOTE_PATH ...]\n",
      "                        Input path arguments to localize into the script's\n",
      "                        execution environment\n",
      "  --input-recursive [KEY=REMOTE_PATH ...]\n",
      "                        Input path arguments to localize recursively into the\n",
      "                        script's execution environment\n",
      "  --output [KEY=REMOTE_PATH ...]\n",
      "                        Output path arguments to de-localize from the script's\n",
      "                        execution environment\n",
      "  --output-recursive [KEY=REMOTE_PATH ...]\n",
      "                        Output path arguments to de-localize recursively from\n",
      "                        the script's execution environment\n",
      "  --user USER, -u USER  User submitting the dsub job, defaults to the current\n",
      "                        OS user.\n",
      "  --user-project USER_PROJECT\n",
      "                        Specify a user project to be billed for all requests\n",
      "                        to Google Cloud Storage (logging, localization,\n",
      "                        delocalization, mounting). This flag exists to support\n",
      "                        accessing Requester Pays buckets (default: None)\n",
      "  --mount [KEY=PATH_SPEC ...]\n",
      "                        Mount a resource such as a bucket, disk, or directory\n",
      "                        into your Docker container\n",
      "  --wait                Wait for the job to finish all its tasks. (default:\n",
      "                        False)\n",
      "  --retries RETRIES     Number of retries to perform on failed tasks.\n",
      "                        (default: 0)\n",
      "  --poll-interval POLL_INTERVAL\n",
      "                        Polling interval (in seconds) for checking job status\n",
      "                        when --wait or --after are set. (default: 10)\n",
      "  --after AFTER [AFTER ...]\n",
      "                        Job ID(s) to wait for before starting this job.\n",
      "  --skip                Do not submit the job if all output specified using\n",
      "                        the --output and --output-recursive parameters already\n",
      "                        exist. Note that wildcard and recursive outputs cannot\n",
      "                        be strictly verified. See the documentation for\n",
      "                        details. (default: False)\n",
      "  --summary             During the --wait loop, display a summary of the\n",
      "                        results, grouped by (job, status). (default: False)\n",
      "  --min-cores MIN_CORES\n",
      "                        Minimum CPU cores for each job. The default is\n",
      "                        provider-specific. The google-cls-v2 provider default\n",
      "                        is 1 core. The local provider does not allocate\n",
      "                        resources, but uses available resources of your\n",
      "                        machine.\n",
      "  --min-ram MIN_RAM     Minimum RAM per job in GB. The default is provider-\n",
      "                        specific. The google-cls-v2 provider default is 3.75\n",
      "                        GB. The local provider does not allocate resources,\n",
      "                        but uses available resources of your machine.\n",
      "  --disk-size DISK_SIZE\n",
      "                        Size (in GB) of data disk to attach for each job\n",
      "                        (default: 200)\n",
      "  --logging LOGGING     Cloud Storage path to send logging output (either a\n",
      "                        folder, or file ending in \".log\")\n",
      "\n",
      "google-common:\n",
      "  Options common to the \"google-cls-v2\" and\n",
      "          \"google-batch\" providers\n",
      "\n",
      "  --project PROJECT     Cloud project ID in which to run the job\n",
      "  --boot-disk-size BOOT_DISK_SIZE\n",
      "                        Size (in GB) of the boot disk (default: 10, 30 for\n",
      "                        google-batch)\n",
      "  --preemptible [PREEMPTIBLE]\n",
      "                        If --preemptible is given without a number, enables\n",
      "                        preemptible VMs for all attempts for all tasks. If a\n",
      "                        number value N is used, enables preemptible VMs for up\n",
      "                        to N attempts for each task. Defaults to not using\n",
      "                        preemptible VMs.\n",
      "  --zones ZONES [ZONES ...]\n",
      "                        List of Google Compute Engine zones.\n",
      "  --scopes SCOPES [SCOPES ...]\n",
      "                        Space-separated scopes for Google Compute Engine\n",
      "                        instances. If unspecified, provider will use 'https://\n",
      "                        www.googleapis.com/auth/bigquery,https://www.googleapi\n",
      "                        s.com/auth/compute,https://www.googleapis.com/auth/dev\n",
      "                        storage.full_control,https://www.googleapis.com/auth/g\n",
      "                        enomics,https://www.googleapis.com/auth/logging.write,\n",
      "                        https://www.googleapis.com/auth/monitoring.write'\n",
      "  --accelerator-type ACCELERATOR_TYPE\n",
      "                        The Compute Engine accelerator type. See\n",
      "                        https://cloud.google.com/compute/docs/gpus/ for\n",
      "                        supported GPU types. Only NVIDIA GPU accelerators are\n",
      "                        currently supported. If an NVIDIA GPU is attached, the\n",
      "                        required runtime libraries will be made available to\n",
      "                        all containers under /usr/local/nvidia. Each version\n",
      "                        of Container-Optimized OS image (used by the Pipelines\n",
      "                        API) has a default supported NVIDIA GPU driver\n",
      "                        version. See https://cloud.google.com/container-\n",
      "                        optimized-os/docs/how-to/run-gpus#install Note that\n",
      "                        attaching a GPU increases the worker VM startup time\n",
      "                        by a few minutes. (default: None)\n",
      "  --accelerator-count ACCELERATOR_COUNT\n",
      "                        The number of accelerators of the specified type to\n",
      "                        attach. By specifying this parameter, you will\n",
      "                        download and install the following third-party\n",
      "                        software onto your job's Compute Engine instances:\n",
      "                        NVIDIA(R) Tesla(R) drivers and NVIDIA(R) CUDA toolkit.\n",
      "                        (default: 0)\n",
      "  --credentials-file CREDENTIALS_FILE\n",
      "                        Path to a local file with JSON credentials for a\n",
      "                        service account.\n",
      "  --regions REGIONS [REGIONS ...]\n",
      "                        List of Google Compute Engine regions. Only one of\n",
      "                        --zones and --regions may be specified.\n",
      "  --machine-type MACHINE_TYPE\n",
      "                        Provider-specific machine type (default: None)\n",
      "  --cpu-platform CPU_PLATFORM\n",
      "                        The CPU platform to request. Supported values can be\n",
      "                        found at https://cloud.google.com/compute/docs/instanc\n",
      "                        es/specify-min-cpu-platform (default: None)\n",
      "  --network NETWORK     The Compute Engine VPC network name to attach the VM's\n",
      "                        network interface to. The value will be prefixed with\n",
      "                        global/networks/ unless it contains a /, in which case\n",
      "                        it is assumed to be a fully specified network resource\n",
      "                        URL. (default: None)\n",
      "  --subnetwork SUBNETWORK\n",
      "                        The name of the Compute Engine subnetwork to attach\n",
      "                        the instance to. (default: None)\n",
      "  --use-private-address\n",
      "                        If set to true, do not attach a public IP address to\n",
      "                        the VM. (default: False)\n",
      "  --timeout TIMEOUT     The maximum amount of time to give the task to\n",
      "                        complete. This includes the time spent waiting for a\n",
      "                        worker to be allocated. Time can be listed using a\n",
      "                        number followed by a unit. Supported units are s\n",
      "                        (seconds), m (minutes), h (hours), d (days), w\n",
      "                        (weeks). The provider-specific default is 7 days.\n",
      "                        Example: '7d' (7 days).\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        The amount of time to sleep between copies of log\n",
      "                        files from the task to the logging path. Time can be\n",
      "                        listed using a number followed by a unit. Supported\n",
      "                        units are s (seconds), m (minutes), h (hours).\n",
      "                        Example: '5m' (5 minutes). Default is '1m'.\n",
      "  --ssh                 If set to true, start an ssh container in the\n",
      "                        background to allow you to log in using SSH and debug\n",
      "                        in real time. (default: False)\n",
      "  --service-account SERVICE_ACCOUNT\n",
      "                        Email address of the service account to be authorized\n",
      "                        on the Compute Engine VM for each job task. If not\n",
      "                        specified, the default Compute Engine service account\n",
      "                        for the project will be used.\n",
      "  --disk-type DISK_TYPE\n",
      "                        The disk type to use for the data disk. Valid values\n",
      "                        are pd-standard pd-ssd and local-ssd. The default\n",
      "                        value is pd-standard.\n",
      "  --enable-stackdriver-monitoring\n",
      "                        If set to true, enables Stackdriver monitoring on the\n",
      "                        VM. (default: False)\n",
      "  --block-external-network\n",
      "                        If set to true, prevents the container for the user's\n",
      "                        script/command from accessing the external network.\n",
      "                        (default: False)\n",
      "  --location LOCATION   Specifies the Google Cloud region to which the\n",
      "                        pipeline request will be sent and where operation\n",
      "                        metadata will be stored. The associated dsub task may\n",
      "                        be executed in another region if the --regions or\n",
      "                        --zones arguments are specified. (default: us-\n",
      "                        central1)\n",
      "\n",
      "Provider-required arguments:\n",
      "  google-batch: ['project', 'logging']\n",
      "  google-cls-v2: ['project', 'logging']\n",
      "  test-fails: []\n",
      "  local: ['logging']\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dsub --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup `cuKING_dsub` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can avoid a lot of unnecessary typing of those default parameter values by using this [bash function](https://linuxize.com/post/bash-functions/) to call dsub.\n",
    "\n",
    "Use of the `cuKING_dsub` function instead of `dsub` is optional but recommended because, in addition to removing a lot of boilerplate code, it also creates a nice folder structure for dsub log files. \n",
    "\n",
    "You can use this `cuKING_dsub` function from both within `%%bash` cells in notebooks and also from the Jupyter terminal. However every cell needs to run `source ~/.bashrc` or `source ~/cuKING_dsub.bash` to utilize the below `cuKING_dsub` function or the cell will not find the `cuKING_dsub` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jupyter/cuKING_dsub.bash\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/cuKING_dsub.bash\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# This shell function passes reasonable defaults for several dsub parameters, while\n",
    "# allowing the caller to override any of them. It creates a nice folder structure within\n",
    "# the workspace bucket for dsub log files.\n",
    "\n",
    "# --[ Parameters ]--\n",
    "# any valid dsub parameter flag\n",
    "\n",
    "#--[ Returns ]--\n",
    "# the job id of the job created by dsub\n",
    "\n",
    "#--[ Details ]--\n",
    "# The first five parameters below should always be those values when running on AoU RWB.\n",
    "\n",
    "# Feel free to change the values for --user, --regions, --logging, and --image if you like.\n",
    "\n",
    "# Note that we insert some job data into the logging path.\n",
    "# https://github.com/DataBiosphere/dsub/blob/main/docs/logging.md#inserting-job-data\n",
    "\n",
    "function cuKING_dsub () {\n",
    "\n",
    "  # Get a shorter username to leave more characters for the job name.\n",
    "  local DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "  # For AoU RWB projects network name is \"network\".\n",
    "  local AOU_NETWORK=network\n",
    "  local AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "  dsub \\\n",
    "      --provider google-cls-v2 \\\n",
    "      --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "      --project \"${GOOGLE_PROJECT}\"\\\n",
    "      --network \"${AOU_NETWORK}\" \\\n",
    "      --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "      --service-account \"$(gcloud config get-value account)\" \\\n",
    "      --image 'us-central1-docker.pkg.dev/broad-mpg-gnomad/images/cuking:v1.0.6' \\\n",
    "      --user \"${DSUB_USER_NAME}\" \\\n",
    "      --regions us-central1 \\\n",
    "      --logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +'%Y%m%d/%H%M%S')/{job-id}-{task-id}-{task-attempt}.log\" \\\n",
    "      \"$@\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make `cuKING_dsub` available from the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add this function to `.bashrc` so that we can easily use it from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo source /home/jupyter/cuKING_dsub.bash >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call cuKING_dsub  --help\n",
    "**Call cuKING_dsub  --help**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: /opt/conda/bin/dsub [-h] [--provider PROVIDER] [--version VERSION]\n",
      "                           [--unique-job-id] [--name NAME]\n",
      "                           [--tasks [FILE M-N ...]] [--image IMAGE]\n",
      "                           [--dry-run] [--command COMMAND] [--script SCRIPT]\n",
      "                           [--env [KEY=VALUE ...]] [--label [KEY=VALUE ...]]\n",
      "                           [--input [KEY=REMOTE_PATH ...]]\n",
      "                           [--input-recursive [KEY=REMOTE_PATH ...]]\n",
      "                           [--output [KEY=REMOTE_PATH ...]]\n",
      "                           [--output-recursive [KEY=REMOTE_PATH ...]]\n",
      "                           [--user USER] [--user-project USER_PROJECT]\n",
      "                           [--mount [KEY=PATH_SPEC ...]] [--wait]\n",
      "                           [--retries RETRIES] [--poll-interval POLL_INTERVAL]\n",
      "                           [--after AFTER [AFTER ...]] [--skip] [--summary]\n",
      "                           [--min-cores MIN_CORES] [--min-ram MIN_RAM]\n",
      "                           [--disk-size DISK_SIZE] [--logging LOGGING]\n",
      "                           [--project PROJECT]\n",
      "                           [--boot-disk-size BOOT_DISK_SIZE]\n",
      "                           [--preemptible [PREEMPTIBLE]]\n",
      "                           [--zones ZONES [ZONES ...]]\n",
      "                           [--scopes SCOPES [SCOPES ...]]\n",
      "                           [--accelerator-type ACCELERATOR_TYPE]\n",
      "                           [--accelerator-count ACCELERATOR_COUNT]\n",
      "                           [--credentials-file CREDENTIALS_FILE]\n",
      "                           [--regions REGIONS [REGIONS ...]]\n",
      "                           [--machine-type MACHINE_TYPE]\n",
      "                           [--cpu-platform CPU_PLATFORM] [--network NETWORK]\n",
      "                           [--subnetwork SUBNETWORK] [--use-private-address]\n",
      "                           [--timeout TIMEOUT] [--log-interval LOG_INTERVAL]\n",
      "                           [--ssh] [--service-account SERVICE_ACCOUNT]\n",
      "                           [--disk-type DISK_TYPE]\n",
      "                           [--enable-stackdriver-monitoring]\n",
      "                           [--block-external-network] [--location LOCATION]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --provider PROVIDER   Job service provider. Valid values are \"google-cls-v2\"\n",
      "                        (Google's Pipelines API v2beta), \"google-batch\"\n",
      "                        (Google's Batch API v1alpha1), and \"local\" (local\n",
      "                        Docker execution). \"test-*\" providers are for testing\n",
      "                        purposes only. (default: google-cls-v2)\n",
      "  --version VERSION, -v VERSION\n",
      "                        Print the dsub version and exit.\n",
      "  --unique-job-id       Experimental: create a unique 32 character UUID for\n",
      "                        the dsub job-id using\n",
      "                        https://docs.python.org/3/library/uuid.html. (default:\n",
      "                        False)\n",
      "  --name NAME           Name for the job. Defaults to the script name or first\n",
      "                        token of the --command if specified.\n",
      "  --tasks [FILE M-N ...]\n",
      "                        Path to a file of tab separated values (TSV) for task\n",
      "                        parameters. The file may be located in the local\n",
      "                        filesystem or in a Google Cloud Storage bucket. The\n",
      "                        first line is a list of column headers specifying an\n",
      "                        --env, --input, --input-recursive, --output or\n",
      "                        --output-recursive variable, and each subsequent line\n",
      "                        specifies the values for a task. Optionally specify\n",
      "                        tasks from the file to submit. Can take the form \"m\",\n",
      "                        \"m-\", or \"m-n\" where m and n are task numbers starting\n",
      "                        at 1. (default: None)\n",
      "  --image IMAGE         Image name from Docker Hub, Google Container\n",
      "                        Repository, or other Docker image service. The task\n",
      "                        must have READ access to the image. (default:\n",
      "                        ubuntu:14.04)\n",
      "  --dry-run             Print the task(s) that would be run and then exit.\n",
      "                        (default: False)\n",
      "  --command COMMAND     Command to run inside the job's Docker container. This\n",
      "                        argument or the --script argument must be provided.\n",
      "  --script SCRIPT       Path to a script that is located in the local file\n",
      "                        system or inside a Google Cloud Storage bucket. This\n",
      "                        script will be run inside the job's Docker container.\n",
      "                        This argument or the --command argument must be\n",
      "                        provided.\n",
      "  --env [KEY=VALUE ...]\n",
      "                        Environment variables for the script's execution\n",
      "                        environment\n",
      "  --label [KEY=VALUE ...]\n",
      "                        Labels to associate to the job.\n",
      "  --input [KEY=REMOTE_PATH ...]\n",
      "                        Input path arguments to localize into the script's\n",
      "                        execution environment\n",
      "  --input-recursive [KEY=REMOTE_PATH ...]\n",
      "                        Input path arguments to localize recursively into the\n",
      "                        script's execution environment\n",
      "  --output [KEY=REMOTE_PATH ...]\n",
      "                        Output path arguments to de-localize from the script's\n",
      "                        execution environment\n",
      "  --output-recursive [KEY=REMOTE_PATH ...]\n",
      "                        Output path arguments to de-localize recursively from\n",
      "                        the script's execution environment\n",
      "  --user USER, -u USER  User submitting the dsub job, defaults to the current\n",
      "                        OS user.\n",
      "  --user-project USER_PROJECT\n",
      "                        Specify a user project to be billed for all requests\n",
      "                        to Google Cloud Storage (logging, localization,\n",
      "                        delocalization, mounting). This flag exists to support\n",
      "                        accessing Requester Pays buckets (default: None)\n",
      "  --mount [KEY=PATH_SPEC ...]\n",
      "                        Mount a resource such as a bucket, disk, or directory\n",
      "                        into your Docker container\n",
      "  --wait                Wait for the job to finish all its tasks. (default:\n",
      "                        False)\n",
      "  --retries RETRIES     Number of retries to perform on failed tasks.\n",
      "                        (default: 0)\n",
      "  --poll-interval POLL_INTERVAL\n",
      "                        Polling interval (in seconds) for checking job status\n",
      "                        when --wait or --after are set. (default: 10)\n",
      "  --after AFTER [AFTER ...]\n",
      "                        Job ID(s) to wait for before starting this job.\n",
      "  --skip                Do not submit the job if all output specified using\n",
      "                        the --output and --output-recursive parameters already\n",
      "                        exist. Note that wildcard and recursive outputs cannot\n",
      "                        be strictly verified. See the documentation for\n",
      "                        details. (default: False)\n",
      "  --summary             During the --wait loop, display a summary of the\n",
      "                        results, grouped by (job, status). (default: False)\n",
      "  --min-cores MIN_CORES\n",
      "                        Minimum CPU cores for each job. The default is\n",
      "                        provider-specific. The google-cls-v2 provider default\n",
      "                        is 1 core. The local provider does not allocate\n",
      "                        resources, but uses available resources of your\n",
      "                        machine.\n",
      "  --min-ram MIN_RAM     Minimum RAM per job in GB. The default is provider-\n",
      "                        specific. The google-cls-v2 provider default is 3.75\n",
      "                        GB. The local provider does not allocate resources,\n",
      "                        but uses available resources of your machine.\n",
      "  --disk-size DISK_SIZE\n",
      "                        Size (in GB) of data disk to attach for each job\n",
      "                        (default: 200)\n",
      "  --logging LOGGING     Cloud Storage path to send logging output (either a\n",
      "                        folder, or file ending in \".log\")\n",
      "\n",
      "google-common:\n",
      "  Options common to the \"google-cls-v2\" and\n",
      "          \"google-batch\" providers\n",
      "\n",
      "  --project PROJECT     Cloud project ID in which to run the job\n",
      "  --boot-disk-size BOOT_DISK_SIZE\n",
      "                        Size (in GB) of the boot disk (default: 10, 30 for\n",
      "                        google-batch)\n",
      "  --preemptible [PREEMPTIBLE]\n",
      "                        If --preemptible is given without a number, enables\n",
      "                        preemptible VMs for all attempts for all tasks. If a\n",
      "                        number value N is used, enables preemptible VMs for up\n",
      "                        to N attempts for each task. Defaults to not using\n",
      "                        preemptible VMs.\n",
      "  --zones ZONES [ZONES ...]\n",
      "                        List of Google Compute Engine zones.\n",
      "  --scopes SCOPES [SCOPES ...]\n",
      "                        Space-separated scopes for Google Compute Engine\n",
      "                        instances. If unspecified, provider will use 'https://\n",
      "                        www.googleapis.com/auth/bigquery,https://www.googleapi\n",
      "                        s.com/auth/compute,https://www.googleapis.com/auth/dev\n",
      "                        storage.full_control,https://www.googleapis.com/auth/g\n",
      "                        enomics,https://www.googleapis.com/auth/logging.write,\n",
      "                        https://www.googleapis.com/auth/monitoring.write'\n",
      "  --accelerator-type ACCELERATOR_TYPE\n",
      "                        The Compute Engine accelerator type. See\n",
      "                        https://cloud.google.com/compute/docs/gpus/ for\n",
      "                        supported GPU types. Only NVIDIA GPU accelerators are\n",
      "                        currently supported. If an NVIDIA GPU is attached, the\n",
      "                        required runtime libraries will be made available to\n",
      "                        all containers under /usr/local/nvidia. Each version\n",
      "                        of Container-Optimized OS image (used by the Pipelines\n",
      "                        API) has a default supported NVIDIA GPU driver\n",
      "                        version. See https://cloud.google.com/container-\n",
      "                        optimized-os/docs/how-to/run-gpus#install Note that\n",
      "                        attaching a GPU increases the worker VM startup time\n",
      "                        by a few minutes. (default: None)\n",
      "  --accelerator-count ACCELERATOR_COUNT\n",
      "                        The number of accelerators of the specified type to\n",
      "                        attach. By specifying this parameter, you will\n",
      "                        download and install the following third-party\n",
      "                        software onto your job's Compute Engine instances:\n",
      "                        NVIDIA(R) Tesla(R) drivers and NVIDIA(R) CUDA toolkit.\n",
      "                        (default: 0)\n",
      "  --credentials-file CREDENTIALS_FILE\n",
      "                        Path to a local file with JSON credentials for a\n",
      "                        service account.\n",
      "  --regions REGIONS [REGIONS ...]\n",
      "                        List of Google Compute Engine regions. Only one of\n",
      "                        --zones and --regions may be specified.\n",
      "  --machine-type MACHINE_TYPE\n",
      "                        Provider-specific machine type (default: None)\n",
      "  --cpu-platform CPU_PLATFORM\n",
      "                        The CPU platform to request. Supported values can be\n",
      "                        found at https://cloud.google.com/compute/docs/instanc\n",
      "                        es/specify-min-cpu-platform (default: None)\n",
      "  --network NETWORK     The Compute Engine VPC network name to attach the VM's\n",
      "                        network interface to. The value will be prefixed with\n",
      "                        global/networks/ unless it contains a /, in which case\n",
      "                        it is assumed to be a fully specified network resource\n",
      "                        URL. (default: None)\n",
      "  --subnetwork SUBNETWORK\n",
      "                        The name of the Compute Engine subnetwork to attach\n",
      "                        the instance to. (default: None)\n",
      "  --use-private-address\n",
      "                        If set to true, do not attach a public IP address to\n",
      "                        the VM. (default: False)\n",
      "  --timeout TIMEOUT     The maximum amount of time to give the task to\n",
      "                        complete. This includes the time spent waiting for a\n",
      "                        worker to be allocated. Time can be listed using a\n",
      "                        number followed by a unit. Supported units are s\n",
      "                        (seconds), m (minutes), h (hours), d (days), w\n",
      "                        (weeks). The provider-specific default is 7 days.\n",
      "                        Example: '7d' (7 days).\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        The amount of time to sleep between copies of log\n",
      "                        files from the task to the logging path. Time can be\n",
      "                        listed using a number followed by a unit. Supported\n",
      "                        units are s (seconds), m (minutes), h (hours).\n",
      "                        Example: '5m' (5 minutes). Default is '1m'.\n",
      "  --ssh                 If set to true, start an ssh container in the\n",
      "                        background to allow you to log in using SSH and debug\n",
      "                        in real time. (default: False)\n",
      "  --service-account SERVICE_ACCOUNT\n",
      "                        Email address of the service account to be authorized\n",
      "                        on the Compute Engine VM for each job task. If not\n",
      "                        specified, the default Compute Engine service account\n",
      "                        for the project will be used.\n",
      "  --disk-type DISK_TYPE\n",
      "                        The disk type to use for the data disk. Valid values\n",
      "                        are pd-standard pd-ssd and local-ssd. The default\n",
      "                        value is pd-standard.\n",
      "  --enable-stackdriver-monitoring\n",
      "                        If set to true, enables Stackdriver monitoring on the\n",
      "                        VM. (default: False)\n",
      "  --block-external-network\n",
      "                        If set to true, prevents the container for the user's\n",
      "                        script/command from accessing the external network.\n",
      "                        (default: False)\n",
      "  --location LOCATION   Specifies the Google Cloud region to which the\n",
      "                        pipeline request will be sent and where operation\n",
      "                        metadata will be stored. The associated dsub task may\n",
      "                        be executed in another region if the --regions or\n",
      "                        --zones arguments are specified. (default: us-\n",
      "                        central1)\n",
      "\n",
      "Provider-required arguments:\n",
      "  google-batch: ['project', 'logging']\n",
      "  google-cls-v2: ['project', 'logging']\n",
      "  test-fails: []\n",
      "  local: ['logging']\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "cuKING_dsub --help | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run all the cells in this notebook, open a **new** terminal and run:\n",
    "```\n",
    "cuKING_dsub --help | more\n",
    "```\n",
    "\n",
    "[If you run this in an existing terminal and get error `aou_dsub: command not found`, just run `source ~/.bashrc` first.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default in this environment, our username is `jupyter` but that's not very informative. There is an environment variable holding our account name that we can use instead as our username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USER_NAME=kchao\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "USER_NAME = os.getenv('OWNER_EMAIL').split('@')[0].replace('.','-')\n",
    "\n",
    "# Save this Python variable as an environment variable so that its easier to use within %%bash cells.\n",
    "%env USER_NAME={USER_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a basic dsub job that succeeds\n",
    "**Run a basic dsub job that succeeds**\n",
    "\n",
    "First, we call `dsub`. This is a simple example that will run a \"Hello World\" job on a Google Cloud VM. It will run on a **different** VM than the one on which this Jupyter notebook is currently running.\n",
    "\n",
    "Logs and output files will be written to the workspace bucket.\n",
    "\n",
    "Notice in the cell below that we use the `cuKING_dsub` function we created previously to set a bunch of default parameter values. The only new parameter values we need to pass are:\n",
    "* `--name` a friendly name for the job\n",
    "* `--output` the path in the workspace bucket where the result will be stored (notice the nice folder structure we create here which is similar to the one in `cuKING_dsub` for `--logging`)\n",
    "* `--command` which in this case is a simple call to `echo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job properties:\n",
      "  job-id: set--kchao--250605-174037-81\n",
      "  job-name: set\n",
      "  user-id: kchao\n",
      "Provider internal-id (operation): projects/417087853780/locations/us-central1/operations/13226008634939225106\n",
      "Launched job-id: set--kchao--250605-174037-81\n",
      "To check the status, run:\n",
      "  dstat --provider google-cls-v2 --project terra-vpc-sc-93ccd8d2 --location us-central1 --jobs 'set--kchao--250605-174037-81' --users 'kchao' --status '*'\n",
      "To cancel the job, run:\n",
      "  ddel --provider google-cls-v2 --project terra-vpc-sc-93ccd8d2 --location us-central1 --jobs 'set--kchao--250605-174037-81' --users 'kchao'\n"
     ]
    }
   ],
   "source": [
    "%%bash --out HELLO_WORLD_JOB_ID\n",
    "\n",
    "source ~/.bashrc # This file was created earlier in this notebook\n",
    "\n",
    "cuKING_dsub \\\n",
    "  --name \"${JOB_NAME}\" \\\n",
    "  --output OUT=\"${WORKSPACE_BUCKET}/dsub/results/${JOB_NAME}/${USER_NAME}/$(date +'%Y%m%d/%H%M%S')/out.txt\" \\\n",
    "  --command 'set -o errexit && \\\n",
    "             set -o xtrace && \\\n",
    "             echo Hello world from the AoU workbench!! > \"${OUT}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_ID=set--kchao--250605-174037-81\n"
     ]
    }
   ],
   "source": [
    "# Save this Python variable value as an environment variable so that its easier to use within %%bash cells.\n",
    "%env JOB_ID={HELLO_WORLD_JOB_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the status of the job\n",
    "**Check the status of the job**\n",
    "\n",
    "You can see in the pink box above that dsub helpfully returns a message saying \"*To check the status, run:*\" and that command above is the same as the one below that uses shell environment variables.\n",
    "\n",
    "Always feel free to copy and run those commands from the dsub output with the exact parameter values. We're just using shell environment variables below so that you can skip that copy/paste step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name    Status                                 Last Update\n",
      "----------  -------------------------------------  --------------\n",
      "set         VM starting (awaiting worker checkin)  06-05 17:40:54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `--full` to get more detail**\n",
    "\n",
    "When you add `--full` you get a lot more detail such as where to find the log files for the run of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- create-time: '2025-06-05 17:40:38.124566'\n",
      "  dsub-version: v0-5-0\n",
      "  end-time: ''\n",
      "  envs: {}\n",
      "  events:\n",
      "  - name: start\n",
      "    start-time: 2025-06-05 17:40:54.304511+00:00\n",
      "  input-recursives: {}\n",
      "  inputs: {}\n",
      "  internal-id: projects/417087853780/locations/us-central1/operations/13226008634939225106\n",
      "  job-id: set--kchao--250605-174037-81\n",
      "  job-name: set\n",
      "  labels: {}\n",
      "  last-update: '2025-06-05 17:40:54.304511'\n",
      "  logging: gs://fc-secure-b25d1307-7763-48b8-8045-fcae9caadfa1/dsub/logs/set/kchao/20250605/174037/set--kchao--250605-174037-81-task-None.log\n",
      "  mounts: {}\n",
      "  output-recursives: {}\n",
      "  outputs:\n",
      "    OUT: gs://fc-secure-b25d1307-7763-48b8-8045-fcae9caadfa1/dsub/results//kchao/20250605/174036/out.txt\n",
      "  provider: google-cls-v2\n",
      "  provider-attributes:\n",
      "    accelerators: []\n",
      "    block-external-network: null\n",
      "    boot-disk-size: 10\n",
      "    cpu_platform: ''\n",
      "    disk-size: 200\n",
      "    disk-type: pd-standard\n",
      "    enable-stackdriver-monitoring: false\n",
      "    instance-name: google-pipelines-worker-e8530c3a779f836f9277ea6184b33dda\n",
      "    machine-type: n1-standard-1\n",
      "    network: network\n",
      "    preemptible: false\n",
      "    regions:\n",
      "    - us-central1\n",
      "    service-account: pet-27156111510312e2a4361@terra-vpc-sc-93ccd8d2.iam.gserviceaccount.com\n",
      "    ssh: false\n",
      "    subnetwork: subnetwork\n",
      "    use_private_address: false\n",
      "    volumes: []\n",
      "    zone: us-central1-b\n",
      "    zones: []\n",
      "  script: |-\n",
      "    #!/usr/bin/env bash\n",
      "    set -o errexit && \\\n",
      "                 set -o xtrace && \\\n",
      "                 echo Hello world from the AoU workbench!! > \"${OUT}\"\n",
      "  script-name: set\n",
      "  start-time: '2025-06-05 17:40:54.304511'\n",
      "  status: RUNNING\n",
      "  status-detail: Worker \"google-pipelines-worker-e8530c3a779f836f9277ea6184b33dda\"\n",
      "    assigned in \"us-central1-b\" on a \"n1-standard-1\" machine\n",
      "  status-message: VM starting (awaiting worker checkin)\n",
      "  user-id: kchao\n",
      "  user-project: terra-vpc-sc-93ccd8d2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*' \\\n",
    "    --full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name    Status                                      Last Update\n",
      "----------  ------------------------------------------  --------------\n",
      "set         Pulling \"gcr.io/google.com/cloudsdktool...  06-05 17:41:31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name    Status    Last Update\n",
      "----------  --------  --------------\n",
      "set         Success   06-05 17:42:21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the result file created by the job**\n",
    "\n",
    "Next, we list a few directories in the workspace bucket to ensure that log files and output files exist.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> the output file will not exist until the job is in <b>status: SUCCESS</b>. You might need to wait a minute or two.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://fc-secure-b25d1307-7763-48b8-8045-fcae9caadfa1/dsub/results//kchao/20250605/174036/out.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil ls \"${WORKSPACE_BUCKET}/dsub/results/${JOB_NAME}/${USER_NAME}/**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from the AoU workbench!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cat \"${WORKSPACE_BUCKET}/dsub/results/${JOB_NAME}/${USER_NAME}/$(date +'%Y%m%d')/*/out.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see\n",
    "```\n",
    "Hello world from the AoU workbench!!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a basic dsub job that fails\n",
    "**Run a basic dsub job that fails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job properties:\n",
      "  job-id: set--kchao--250605-174339-51\n",
      "  job-name: set\n",
      "  user-id: kchao\n",
      "Provider internal-id (operation): projects/417087853780/locations/us-central1/operations/968185655538755317\n",
      "Launched job-id: set--kchao--250605-174339-51\n",
      "To check the status, run:\n",
      "  dstat --provider google-cls-v2 --project terra-vpc-sc-93ccd8d2 --location us-central1 --jobs 'set--kchao--250605-174339-51' --users 'kchao' --status '*'\n",
      "To cancel the job, run:\n",
      "  ddel --provider google-cls-v2 --project terra-vpc-sc-93ccd8d2 --location us-central1 --jobs 'set--kchao--250605-174339-51' --users 'kchao'\n"
     ]
    }
   ],
   "source": [
    "%%bash --out HELLO_WORLD_JOB_ID\n",
    "\n",
    "source ~/cuKING_dsub.bash # This file was created via notebook 01_dsub_setup.ipynb.\n",
    "\n",
    "cuKING_dsub \\\n",
    "  --name \"${JOB_NAME}\" \\\n",
    "  --output OUT=\"${WORKSPACE_BUCKET}/dsub/results/${JOB_NAME}/${USER_NAME}/$(date +'%Y%m%d/%H%M%S')/out.txt\" \\\n",
    "  --command 'set -o errexit && \\\n",
    "             set -o xtrace && \\\n",
    "             echo \"This job fails because no input is passed to cuKING\" && \\\n",
    "             cuking > \"${OUT}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_ID=set--kchao--250605-174339-51\n"
     ]
    }
   ],
   "source": [
    "# Save this Python variable value as an environment variable so that its easier to use within %%bash cells.\n",
    "%env JOB_ID={HELLO_WORLD_JOB_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name    Status    Last Update\n",
      "----------  --------  --------------\n",
      "set         Pending   06-05 17:43:39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*' \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name    Status                                      Last Update\n",
      "----------  ------------------------------------------  --------------\n",
      "set         Stopped running \"user-command\": exit st...  06-05 17:45:13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the error message in the log file\n",
    "**Take a look at the error message in the log file**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> the log files may not exist until the job is in <b>status: FAILURE</b>. You might need to wait a minute or two.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> If you have waited for the job to complete, you will see <b>two sets of log files</b>: one set from the successful run and the other set from the failed run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: gs://fc-secure-b25d1307-7763-48b8-8045-fcae9caadfa1/dsub/logs//kchao/20250605/*/set--kchao--250605-174339-51*.log\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ngsutil cat \"${WORKSPACE_BUCKET}/dsub/logs/${JOB_NAME}/${USER_NAME}/$(date +\\'%Y%m%d\\')/*/${JOB_ID}*.log\"\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mgsutil cat \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;132;43;01m{WORKSPACE_BUCKET}\u001b[39;49;00m\u001b[38;5;124;43m/dsub/logs/$\u001b[39;49m\u001b[38;5;132;43;01m{JOB_NAME}\u001b[39;49;00m\u001b[38;5;124;43m/$\u001b[39;49m\u001b[38;5;132;43;01m{USER_NAME}\u001b[39;49;00m\u001b[38;5;124;43m/$(date +\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)/*/$\u001b[39;49m\u001b[38;5;132;43;01m{JOB_ID}\u001b[39;49;00m\u001b[38;5;124;43m*.log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ngsutil cat \"${WORKSPACE_BUCKET}/dsub/logs/${JOB_NAME}/${USER_NAME}/$(date +\\'%Y%m%d\\')/*/${JOB_ID}*.log\"\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cat \"${WORKSPACE_BUCKET}/dsub/logs/${JOB_NAME}/${USER_NAME}/$(date +'%Y%m%d')/*/${JOB_ID}*.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the message from the successful `echo` command that occurred before the error:\n",
    "```\n",
    "This job fails because no input is passed to cuKING\n",
    "```\n",
    "\n",
    "**Take a look at what was sent to [stderr](http://www.linfo.org/standard_error.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: gs://fc-secure-b25d1307-7763-48b8-8045-fcae9caadfa1/dsub/logs//kchao/20250605/*/set--kchao--250605-174339-51*-stderr.log\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ngsutil cat \"${WORKSPACE_BUCKET}/dsub/logs/${JOB_NAME}/${USER_NAME}/$(date +\\'%Y%m%d\\')/*/${JOB_ID}*-stderr.log\"\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mgsutil cat \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;132;43;01m{WORKSPACE_BUCKET}\u001b[39;49;00m\u001b[38;5;124;43m/dsub/logs/$\u001b[39;49m\u001b[38;5;132;43;01m{JOB_NAME}\u001b[39;49;00m\u001b[38;5;124;43m/$\u001b[39;49m\u001b[38;5;132;43;01m{USER_NAME}\u001b[39;49;00m\u001b[38;5;124;43m/$(date +\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)/*/$\u001b[39;49m\u001b[38;5;132;43;01m{JOB_ID}\u001b[39;49;00m\u001b[38;5;124;43m*-stderr.log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ngsutil cat \"${WORKSPACE_BUCKET}/dsub/logs/${JOB_NAME}/${USER_NAME}/$(date +\\'%Y%m%d\\')/*/${JOB_ID}*-stderr.log\"\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cat \"${WORKSPACE_BUCKET}/dsub/logs/${JOB_NAME}/${USER_NAME}/$(date +'%Y%m%d')/*/${JOB_ID}*-stderr.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the error message:\n",
    "```\n",
    "Error: INVALID_ARGUMENT: No input URI specified\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "317px",
    "width": "207px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
